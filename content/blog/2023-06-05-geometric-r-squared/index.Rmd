---
title: A (differential) geometric interpretation of the coefficient of determination
author: Corson N. Areshenkoff
date: '2023-06-05'
slug: geometric-rsquared
categories: []
tags: []
description: ''
weight: 20
---

```{r opts, echo = FALSE}
knitr::opts_chunk$set(
  fig.path = "img/"
)
```

Nothing that I discuss here is particularly deep, or new. In fact, this way of interpreting $R^2$ is needlessly convoluted, but it provides a relatively simple and intuitive introduction to some concepts that require much more sophisticated mathematical machinery in dimensions greater than one. It also provides a fairly intuitive -- and, most importantly, visualizeable -- example of some constructions used in more

Given two vectors $X$ and $Y$, the angle $\theta$ between them is given by

$$ \theta = \text{acos} \left ( \frac{X \cdot Y}{\|X\|\|Y\|} \right ).$$

The cosine of this angle is often used as a measure of similarity (appropriately named "cosine similarity"), and has the convenient property of being bounded between $1$ and $-1$, denoting vectors which point in the same or opposite directions, respectively. Note that this expression can be written in the slightly more familiar form 

$$ \cos{\theta} = \left ( \frac{\sum_i x_iy_i}
  {\sqrt{\sum_ix_i^2} \sqrt{\sum_iy_i^2}} \right ).$$
  
Consider now the Pearson correlation between two centered vectors (the centering makes no difference except to make the algebra slightly easier):

\begin{align} 
  r_{XY} &= \frac{\sigma_{XY}}{\sigma_X \sigma_Y} \\
         &= \frac{\sum_i{x_iy_i}}
         {\sqrt{\sum_i x^2}\sqrt{\sum_i y^2}} \\
         &= \cos{\theta},
\end{align}

which provides a well known and convenient geometric interpretation of the correlation: Given two (centered) vectors $X$ and $Y$, the correlation between them is the cosine of the angle between them. When they point in the same direction, the correlation is high; when the point in opposite directions, the correlation is negative; and when they point at a right angle, the correlation is zero.

Notice (obviously) that the correlation is a *signed* measure -- it cares whether $X$ points in the *same* or in the *opposite* direction of $Y$; but there are cases in which we do not care about this distinction. Here are two examples, which lead to different notions of similarity which turn out to be fundamentally related: 

(1) A researcher is interested in *predicting* Y using X -- say, with a simple linear regression model. The fundamental concern is thus the *strength* of the relationship, rather than its specific sign. This is often summarized using the coefficient of determination, which has several equivalent interpretations which we will discuss shortly.

(2) An example which sounds contrived in this setting, but which I am currently dealing with in the context of Human neuroimaging data: Suppose that we have conducted principal component analysis on two sets of data, and have retained only the dominant component in each case. How consistent are the results of the two analyses? *Directly* comparing the components is tricky, because the sign of the component is arbitrary (multiplying both the loading and component scores by $-1$ changes nothing about the results). We thus want some kind of measure of similarity that doesn't distinguish between the components pointing in the same or in the opposite directions.

### (1) The coefficient of determination

### (2) Similar subspaces



