---
title: A (differential) geometric interpretation of the coefficient of determination
author: Corson N. Areshenkoff
date: '2023-06-05'
slug: geometric-rsquared
categories: []
tags: []
description: ''
weight: 20
css: envblocks.css
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>
  <link rel="stylesheet" href="envblocks.css" type="text/css" />


<p>Note that nothing that I discuss here is particularly deep, or new. In fact, this way of interpreting <span class="math inline">\(R^2\)</span> is needlessly convoluted, but it provides a relatively simple and intuitive introduction to some concepts that require much more sophisticated mathematical machinery in dimensions greater than one. It also provides a fairly intuitive – and, most importantly, visualizeable – example of some constructions used in more sophisticated applications.</p>
<p>Given two vectors <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the angle <span class="math inline">\(\theta\)</span> between them is given by</p>
<p><span class="math display">\[ \theta = \text{acos} \left ( \frac{X \cdot Y}{\|X\|\|Y\|} \right ).\]</span></p>
<p>The cosine of this angle is often used as a measure of similarity (appropriately named “cosine similarity”), and has the convenient property of being bounded between <span class="math inline">\(1\)</span> and <span class="math inline">\(-1\)</span>, denoting vectors which point in the same or opposite directions, respectively. Note that this expression can be written in the slightly more familiar form</p>
<p><span class="math display">\[ \cos{\theta} =  \frac{\sum_i x_iy_i}
  {\sqrt{\sum_ix_i^2} \sqrt{\sum_iy_i^2}} .\]</span></p>
<p>Consider now the Pearson correlation between two centered vectors (the centering makes no difference except to make the algebra slightly easier):</p>
<p><span class="math display">\[\begin{align}
  r_{XY} &amp;= \frac{\sigma_{XY}}{\sigma_X \sigma_Y} \\
         &amp;= \frac{\sum_i{x_iy_i}}
         {\sqrt{\sum_i x^2}\sqrt{\sum_i y^2}} \\
         &amp;= \cos{\theta},
\end{align}\]</span></p>
<p>which provides a well known and convenient geometric interpretation of the correlation: Given two (centered) vectors <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the correlation between them is the cosine of the angle between them. When they point in the same direction, the correlation is high; when the point in opposite directions, the correlation is negative; and when they point at a right angle, the correlation is zero.</p>
<p>Notice (obviously) that the correlation is a <em>signed</em> measure – it cares whether <span class="math inline">\(X\)</span> points in the <em>same</em> or in the <em>opposite</em> direction of <span class="math inline">\(Y\)</span>; but there are cases in which we do not care about this distinction. Here are two examples, which lead to different notions of similarity which turn out to be fundamentally related:</p>
<ol style="list-style-type: decimal">
<li><p>A researcher is interested in <em>predicting</em> Y using X – say, with a simple linear regression model. The fundamental concern is thus the <em>strength</em> of the relationship, rather than its specific sign. This is often summarized using the coefficient of determination, which has several equivalent interpretations which we will discuss shortly.</p></li>
<li><p>An example which sounds contrived in this setting, but which I am currently dealing with in the context of Human neuroimaging data: Suppose that we have conducted principal component analysis on two sets of data, and have retained only the dominant component in each case. How consistent are the results of the two analyses? <em>Directly</em> comparing the components is tricky, because the sign of the component is arbitrary (multiplying both the loading and component scores by <span class="math inline">\(-1\)</span> changes nothing about the results). We thus want some kind of measure of similarity that doesn’t distinguish between the components pointing in the same or in the opposite directions.</p></li>
</ol>
<p><img src="img/correlation-angle.png" width="90%" /></p>
<p>Note that <span class="math inline">\(R^2\)</span> has two principal interpretations, which are equivalent for linear models: (1) It is the proportion of variance explained by the model; and (2) it is the squared correlation between the actual and predicted outcomes. It is the second interpretation which we will focus on here.</p>
<p><img src="img/coangles.png" width="30%" style="float:left; padding:10px" /></p>
<p>The distance between the two subspaces on the circle is the <em>smallest</em> of the two angles (seen in blue on the left). If this angle is <span class="math inline">\(\theta\)</span> – corresponding to a correlation of <span class="math inline">\(r = \cos{\theta}\)</span> – then the complementary (longer; in red) angle is <span class="math inline">\(\pi - \theta\)</span>. But this just corresponds to a correlation of <span class="math inline">\(\cos{(\pi - \theta)} = -\cos{\theta} = -r\)</span>. Hence, the squared correlation is indifferent to this selection – it is simply the squared geodesic distance between the subspaces spanned by the two vectors <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The distance itself is, of course, simply the absolute value of the correlation.</p>
<p>This metaphor is fun. Let’s run with it.</p>
<p>What about multiple regression? <span class="math inline">\(R^2\)</span> has the same interpretation here – the squared correlation between the actual and predicted response, but here the predicted response is not simply a translation and scaling of a single predictor, so the <span class="math inline">\(R^2\)</span> says nothing about the direct relationship between the outcome <span class="math inline">\(Y\)</span> and the predictors <span class="math inline">\((X_1,X_2, ..., X_n)\)</span>. In this case, we can appeal to a geometric interpretation of multiple regression: The least-squares predictions are the (orthogonal) projection of the dependent variable onto the subspace spanned by the predictors. In that case <span class="math inline">\(R^2\)</span> is the (squared) distance between the subspace spanned by the response, and the subspace spanned by its projection onto the subspace spanned by the predictors (i.e. the nearest subspace contained in the span of the predictors).</p>
<p>Note that this construction is utterly indifferent to even perfect multicolinearity: Computing <span class="math inline">\(R^2\)</span> in this way requires only the response, and its projection onto the subspace spanned by the predictors. As such, it does not care about the <em>coefficients</em> (which are not uniquely defined when the predictors are linearly dependent).</p>
<div id="multivariate-regression" class="section level3">
<h3>Multivariate regression</h3>
<p>This interpretation of <span class="math inline">\(R^2\)</span> suggests a straightforward extension to the multivariate case. Here, we have the following data: A matrix of outcomes <span class="math inline">\(\bf{Y}\)</span>, whose columns span some subspace <span class="math inline">\(\text{Sp}(\bf{Y})\)</span>, and a matrix of predicted values <span class="math inline">\(\bf{\hat{Y}}\)</span>, whose columns span some subspace <span class="math inline">\(\text{Sp}(\bf{\hat{Y}})\)</span>. As an analogy with the univariate case, in which we considered the distance between one-dimensional subspaces (lines), what can we say about the distance between <span class="math inline">\(\text{Sp}(\bf{Y})\)</span> and <span class="math inline">\(\text{Sp}(\bf{\hat{Y}})\)</span>, which may be higher dimensional subspaces (e.g. planes)?</p>
<p>There is a straightforward, principled way of doing this when the subspaces are of equal dimension, but that’s not always (or even often) the case. Nevertheless, it’s still useful tp</p>
<div class="definition-p" data-label="Principal angle">
<p>The purpose of principal angles is to quantify the similarity between two subspaces, such as two planes through the origin. For those unfamiliar with the concept, here is an almost rigorous definition:</p>
<ol style="list-style-type: decimal">
<li><p>Given two k-dimensional subspaces <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, pick two vectors <span class="math inline">\(x_1\)</span> and <span class="math inline">\(y_1\)</span> in each respective subspace which have the smallest possible angle between them. Call this angle <span class="math inline">\(\theta_1\)</span>.</p></li>
<li><p>Now, find another pair of vectors <span class="math inline">\(x_2\)</span> and <span class="math inline">\(y_2\)</span> with the smallest possible angle between them, subject to the constraint that <span class="math inline">\(x_2\)</span> is orthogonal to <span class="math inline">\(x_1\)</span>, and similarly for <span class="math inline">\(y\)</span>. Call this <span class="math inline">\(\theta_2\)</span>.</p></li>
<li><p>Repeat <span class="math inline">\(k\)</span> times.</p></li>
</ol>
<p>The result is an increasing sequence of <em>principal angles</em> (PAs). Note that if the two subspaces intersect, then at least one principal angle must be zero, since we can then take the same vector in each subspace. More generally, the number of zero principal angles is equal to the dimension of the intersection. Given two distinct planes through the origin, whose intersection must be a line, we will then have one zero PA and another giving the angle at which one plane is tilted relative to another. Note also that in the one-dimensional case, the principal angle reduces to the angle between two lines.</p>
<p>Note that this construction works perfectly well for subspaces of unequal dimension. If <span class="math inline">\(X\)</span> is <span class="math inline">\(k\)</span>-dimensional and <span class="math inline">\(Y\)</span> is <span class="math inline">\(l\)</span>-dimensional, then we can extract <span class="math inline">\(\text{min}(k,l)\)</span> principal angles.</p>
<p>To compute these angles, let <span class="math inline">\(\bf{U}\)</span> and <span class="math inline">\(\bf{V}\)</span> be matrices whose columns are orthonormal bases for <span class="math inline">\(\text{Sp}(X)\)</span> and <span class="math inline">\(\text{Sp}(Y)\)</span>, respectively. Then we have</p>
<p><span class="math display">\[\begin{equation}
\cos{\theta_i} = \sigma_i(U^{\top}V)
\end{equation}\]</span>
where <span class="math inline">\(\sigma_i\)</span> denotes the <span class="math inline">\(i\)</span>’th singular value.</p>
</div>
</div>
