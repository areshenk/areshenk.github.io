[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nMar 11, 2024\n\n\nSynergizing reinforcement learning with functional deep quantum neural manifold learning\n\n\n\n\nSep 26, 2023\n\n\nChatGPT experiment log pt. 1: Learning basic statistics\n\n\n\n\nMay 26, 2019\n\n\nSpurious dynamics in functional brain networks\n\n\n\n\nJan 16, 2019\n\n\nMore spurious modular structure in functional brain networks\n\n\n\n\nDec 10, 2018\n\n\nSpurious modular structure in functional brain networks\n\n\n\n\nSep 9, 2018\n\n\nIn a room sit three textbooks…\n\n\n\n\nFeb 18, 2018\n\n\nCargo cult statistics\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-09-26-chatgpt-stats-pt1/index.html",
    "href": "posts/2023-09-26-chatgpt-stats-pt1/index.html",
    "title": "ChatGPT experiment log pt. 1: Learning basic statistics",
    "section": "",
    "text": "It has to be good for something, right? I thought it would be fun to experiment with some tasks for which ChatGPT is widely used in data science; or, at least, for which I’ve seen it used by students in data science / statistics chatrooms and forums. One of the most common – and for which examples can be found everywhere beginning data scientists congregate – is the use of ChatGPT to design an analysis, answer questions about statistics, or more generally to provide knowledge.\nOne of the major difficulties in evaluating resources like ChatGPT as a teacher is that 1) Without already having detailed technical knowledge, it’s hard to evaluate the accuracy of its output, and 2) With detailed knowledge and experience, it’s too easy to (unconsciously) fill in the gaps in ChatGPT’s output, or to interpret the output correctly in a way that a novice would probably not. Here’s a simple example of the kind of use I’ve seen recommended to novices trying to pick up on the basics of data analysis:\n\n\n\n\n\n\nPrompt\n\n\n\nHow do I analyze two continuous variables?\n\n\nThis question is more than a bit ill-posed, as I haven’t actually explained anything about the nature of the data, or the specific research question\n\n\n\n\n\n\nChatGPT output\n\n\n\nAnalyzing two continuous variables typically involves examining the relationship between them. Here are the common methods to analyze two continuous variables:\n\nDescriptive Statistics:\n\nCalculate the mean, median, mode, standard deviation, and range for each variable.\nThis gives a sense of the central tendency and spread of each variable.\n\nGraphical Analysis:\n\nScatter Plot: Plot one variable against the other to visually inspect the relationship.\nHistograms: For each variable, to see the distribution.\nBox Plots: For each variable, to visualize the central tendency and spread, as well as any outliers.\n\nCorrelation Analysis:\n\nPearson’s Correlation Coefficient (r): Measures the strength and direction of a linear relationship between two variables. The value ranges between -1 and 1. A value close to 1 implies a strong positive correlation, a value close to -1 implies a strong negative correlation, and a value close to 0 implies no correlation.\nSpearman’s Rank Correlation: Used when the relationship is not linear or the data is not normally distributed.\n\nRegression Analysis:\n\nSimple Linear Regression: If you suspect one variable might predict another, you can fit a linear regression model. This gives an equation of the form \\(y = mx + c\\), where \\(y\\) is the dependent variable, \\(x\\) is the independent variable, \\(m\\) is the slope, and \\(c\\) is the y-intercept.\nMultiple Regression: If there are other variables that might influence the relationship, you can include them in a multiple regression analysis.\n\nNon-parametric Tests:\n\nIf the data doesn’t meet the assumptions of normality, you might consider non-parametric tests like the Mann-Whitney U test or the Kruskal-Wallis test.\n\nResidual Analysis:\n\nAfter fitting a regression model, it’s important to check the residuals (the differences between observed and predicted values) to ensure that the assumptions of the model are met.\n\nCovariance Analysis:\n\nCovariance measures the joint variability of two random variables. If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values, the covariance is positive.\n\nCross-tabulation and Chi-square Test:\n\nAlthough typically used for categorical data, continuous data can be binned into categories, and a chi-square test can be used to determine if there’s a significant association between the two variables.\n\n\nWhen analyzing two continuous variables, it’s crucial to consider the context and the nature of the data. Not all methods will be appropriate for all datasets, and sometimes domain knowledge will guide the choice of analysis.\n\n\nThis is actually a fairly reasonable survey of the most common approaches to plotting, summarizing, or quantifying the association between two continuous variables. The boilerplate text at the bottom of the output is reasonable, too, since I haven’t actually explained anything about the goals of the analysis. Someone with little knowledge of basic statistics could actually get a useful sense of how to proceed with their analysis. For example, if I were interested in using one of the variables to predict the other, the output clearly suggests that something like a simple linear regression model, or the correlation/covariance, would be useful.\nUnfortunately, many of the specific details – and some of the overall organization – is/are nonsense, and are nonsense in ways that would very possibly lead a novice to make nonsensical decisions. Examples:\n\nChatGPT describes the Spearman correlation as being used “when the relationship is not linear or the data is not normally distributed”. The first part is true: the Spearman correlation quantifies general monotonic relationships, but the second part is nonsense. Nothing about the interpretation of the Pearson correlation requires normality, and most tests of the Pearson correlation do not assume normality of any individual variable. The standard t-test for the correlation assumes only that the errors are normal when one variable is regressed on the other. If you’re not willing to assume any kind of normality at all, then you can just use a permutation test. The decision as to which measure to use is dictated by the kinds of relationship the user is trying to quantify, not any distributional assumptions.\nThat the chi-squared test can be used when “continuous data [are] binned into categories […] to determine if there’s a significant association between the two variables”. This is strictly true, but is essentially always a terrible idea. It is nearly always less powerful than a model actually adapted to continuous data, and the association that it is testing for (i.e. general non-independence) rarely answers the question most people actually have about multiple continuous variables. All in all, just a useless suggestion.\n“If the data doesn’t meet the assumptions of normality, you might consider non-parametric tests like the Mann-Whitney U test or the Kruskal-Wallis test.”. This is a bizarre suggestion, since the MW and KW are not “alternatives” to any other technique on the list. That is, there is nothing at all on the list that a) assumes normality, and b) could be replaced with either of these tests in a way that actually answers the same research question.\n\nNumber (1) is a very common misconception in introductory statistics and data science course materials, and in articles on platforms like medium, and so it seems likely that ChatGPT is just regurgitating common misinformation. The same is true of (3), where the MW and KW are often presented as “non-parametric alternatives” to the t-test, even though they test completely different hypotheses (stochastic equality vs. mean equality). Often – as in this ChatGPT output – the claim is that these tests should be used instead of the t-test when the data are non-normal, even though there are other alternatives (like permutation tests) which do not assume normality and actually answer the same question.\nNumber (3) also lacks context; the MW/KW are recommended “when the data are non-normal”, but it’s not explained what they’re supposed to be replacing, and none of the other procedures actually outlined by ChatGPT assume or require normality of any of the observed variables (only linear regression typically assumes normality of the errors). I think this is just an example of ChatGPT being skilled at generating text it has seen before, but not in generating output with a coherent/ logical organization. Something like this could easily lead someone to plot a few of their variables, note that they don’t look normal, and choose one of these tests even though another procedure might be perfectly appropriate, and might more directly address their actual research question.\nSpeaking of the Mann-Whitney:\n\n\n\n\n\n\nPrompt\n\n\n\nWhen should I use the Mann-Whitney test?\n\n\n\n\n\n\n\n\nChatGPT output\n\n\n\nThe Mann-Whitney U test, also known as the Wilcoxon rank-sum test, is a non-parametric statistical test used to determine if there are differences between two independent groups on a continuous or ordinal variable. Here are the situations when you should consider using the Mann-Whitney U test:\n[…]\n\nComparing Central Tendencies:\n\nThe Mann-Whitney U test compares the medians of two groups rather than the means. It tests the null hypothesis that the probability of observing a randomly selected value from the first group that is larger than a randomly selected value from the second group is equal to 0.5.\n\n\n[…]\n\nAssumption of Similar Shapes:\n\nFor the Mann-Whitney U test to be a test of medians, the distributions of the two groups should have the same shape. If the shapes are different, the test is more of a test of stochastic dominance (i.e., the likelihood that a randomly selected value from one distribution will be greater than a randomly selected value from the other distribution).\n\n\nIn summary, the Mann-Whitney U test is a versatile non-parametric test suitable for comparing two independent groups when the assumptions of parametric tests (like the t-test) are not met. However, as with all statistical tests, it’s essential to understand its assumptions and limitations before using it.\n\n\nThat’s pretty reasonable. I was expecting the incorrect information (which is actually provided in 4.) that the MW is a test of medians (which is very commonly reported in introductory textbooks and online material), but then the output corrects itself in (7) and reports that it is only a test of medians when the alternative is a pure location shift. Minor nitpick: “Likelihood” is the wrong word here. “Probability” is correct. But so what? ChatGPT is just being colloquial.\nIt is interesting, though, that ChatGPT provides both the wrong and the right answer in the same output. It also gives the correct definition of stochastic equality in (4) – this is the actual null hypothesis – but claims in the same breath that the MW is a test of medians. This is a good example of ChatGPT stringing together realistic sounding output, but not being able to reason about the technical meaning of its words.\n\n\n\n\n\n\nPrompt\n\n\n\nCan you give me a simple description of linear regression?\n\n\n\n\n\n\n\n\nChatGPT output\n\n\n\nCertainly!\nLinear Regression is a statistical method that models the relationship between a dependent variable and one or more independent variables by fitting a linear equation to the observed data. The simplest form of linear regression, with one dependent and one independent variable, is represented by the equation:\n\\[ y = \\beta_0 + \\beta_1x + \\epsilon \\]\nWhere:\n\n\\(y\\) is the dependent variable (what we’re trying to predict).\n\\(x\\) is the independent variable (the input).\n\\(\\beta_0\\) is the y-intercept (value of \\(y\\) when \\(x\\) is 0).\n\\(\\beta_1\\) is the slope of the line (how much \\(y\\) changes for a unit change in \\(x\\)).\n\\(\\epsilon\\) represents the error term (the difference between the observed and predicted values).\n\nThe primary goal of linear regression is to find the best-fitting straight line that accurately predicts the output values within a range.\n\n\nNot bad. ChatGPT gives the wrong definition of the error term (what it describes is actually the residuals), but the overall explanation is pretty clear and intuitive. This sounds more or less like what we want. So how do I actually perform a regression analysis?\n\n\n\n\n\n\nPrompt\n\n\n\nHow do I use linear regression to predict one variable using another?\n\n\n\n\n\n\n\n\nChatGPT output\n\n\n\nUsing linear regression to predict one variable using another involves establishing a linear relationship between the two variables. Here’s a step-by-step guide using Python with the statsmodels and matplotlib libraries:\n\nImport Necessary Libraries:\n\nimport numpy as np\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nPrepare the Data: Assuming you have two lists or arrays, X (independent variable) and y (dependent variable):\n\n# Sample data\nX = [1, 2, 3, 4, 5]\ny = [2, 4, 5, 4, 5]\n\nAdd a Constant to the Independent Variable: This step is necessary to include an intercept in the regression model.\n\nX = sm.add_constant(X)\n\nFit the Linear Regression Model:\n\nmodel = sm.OLS(y, X).fit()\n\nPrint the Summary: This will give you details about the regression, including coefficients, R-squared value, p-values, etc.\n\nprint(model.summary())\n\nMake Predictions: You can use the fitted model to make predictions for new values of the independent variable.\n\nnew_X = [[1, 6]]  # Predicting y for X=6\npredicted_y = model.predict(new_X)\nprint(f\"Predicted value of y for X=6 is {predicted_y[0]}\")\n\nVisualize the Regression Line:\n\nplt.scatter(X[:, 1], y, color='blue', label='Data Points')\nplt.plot(X[:, 1], model.predict(X), color='red', label='Regression Line')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Linear Regression')\nplt.legend()\nplt.show()\nThis will plot the data points and the regression line, helping you visualize the linear relationship between the two variables.\nRemember to install the required libraries (statsmodels and matplotlib) before running the code. This example demonstrates simple linear regression with one independent variable. If you have multiple independent variables, the process remains largely the same, but you’ll need to adjust the data preparation and visualization steps accordingly.\n\n\nThis is pretty spectacular, honestly. If you know enough to validate the model output (to ensure that the code is running correctly), then this would be a fantastic way for someone already proficient in statistics, and comfortable with a language like R, to learn statistical programming in Python. But suppose we weren’t already knowledgeable, and we wanted to know more about whether this model is reasonable at all.\n\n\n\n\n\n\nPrompt\n\n\n\nHow do I know if linear regression is right for my dataset?\n\n\n\n\n\n\n\n\nChatGPT output\n\n\n\nDetermining whether linear regression is appropriate for your dataset involves checking several conditions and considering the nature of your data and research question. Here are some steps to help you decide:\n\nDefine the Research Question:\n\nLinear regression is suitable for understanding the relationship between one dependent (response) variable and one or more independent (predictor) variables. Ensure your research question aligns with this.\n\nVisualize the Data:\n\nCreate scatter plots of the dependent variable against each independent variable.\nLook for a linear trend in the scatter plots. If the relationship appears non-linear, a linear model may not be the best choice.\n\nCheck Linearity Assumption:\n\nLinear regression assumes a linear relationship between the dependent and independent variables.\nIf the relationship is inherently non-linear, consider transforming the variables or using non-linear regression models.\n\nExamine the Distribution of the Dependent Variable:\n\nWhile linear regression does not require the dependent variable to be normally distributed, extreme skewness or outliers can impact the model. Consider data transformations if necessary.\n\nCheck for Multicollinearity (for Multiple Linear Regression):\n\nMulticollinearity occurs when independent variables are highly correlated, which can affect the model’s stability and interpretability.\nCalculate the Variance Inflation Factor (VIF) for each independent variable. A VIF above 5-10 indicates high multicollinearity.\n\nAssess Homoscedasticity:\n\nThe variance of the residuals should be constant across all levels of the independent variables.\nPlot residuals against fitted values and look for any patterns. Heteroscedasticity (non-constant variance) suggests that linear regression may not be suitable.\n\nCheck Independence of Residuals:\n\nThe residuals (errors) should be independent.\nFor time series data, plot residuals over time to check for autocorrelation.\n\nConsider the Sample Size:\n\nLinear regression requires a sufficient sample size, especially for multiple linear regression with several independent variables.\nA general rule of thumb is having at least 20 observations per independent variable.\n\nEvaluate Model Assumptions and Fit:\n\nAfter fitting a linear regression model, assess the model assumptions (linearity, independence, homoscedasticity, normality of residuals) and overall model fit.\nConsider alternative models if the assumptions are violated and cannot be remedied with data transformations.\n\nDomain Knowledge:\n\n\nConsider any domain-specific knowledge or theoretical framework that might indicate whether a linear relationship is plausible.\n\nIf your dataset does not meet the assumptions or linear regression does not align with your research question, you may need to consider alternative modeling techniques such as polynomial regression, logistic regression, or non-parametric methods.\n\n\nThis is a pretty reasonable summary of the basic issues a researcher might consider when deciding on whether to fit a linear regression model. The only thing I really disagree with is item (4): The distribution of the dependent (or the independent) variable is completely irrelevant to the assumptions of the model; the only reason you might worry about something like skewness is because outlying points might have high leverage, but I wouldn’t generally deal with that by transforming anything. If the original relationship was linear, it won’t be linear after transformation.\nThere are also some minor points of contention, like how (5) describes multicolinearity as “when the dependent variables are highly correlated”. This isn’t strictly true; multicolinearity occurs when the predictors are linearly dependent, and can happen even when the correlation between any two predictors is very small. This isn’t a huge issue, and I’d wager that most introductory material on the internet frames multicolinearity in this way (which is probably why ChatGPT does as well). At least it recommends an approach based on the VIF rather than directly examining the covariance, but I could easily imagine a naive user reading this and choosing to diagnose multicolinearity by directly examining a correlation matrix.\nItem (6) says that, in the presence of heteroskedasticity, linear regression might not be suitable. I think this is a bit extreme: There are plenty of heteroskedasticity-robust approaches to doing inference; and if there is a reasonable model for the heteroskedasticity, then weighted least-squares is always an option. Let’s see what ChatGPT says:\n\n\n\n\n\n\nPrompt\n\n\n\nWhat are alternatives to linear regression when the residuals are heteroskedastic?\n\n\n\n\n\n\n\n\nChatGPT output\n\n\n\nHeteroskedasticity, the condition where the variance of the residuals is not constant across levels of an independent variable, can violate the assumptions of linear regression and potentially lead to incorrect conclusions. Here are some alternatives and remedies when facing heteroskedasticity:\n\nTransformation of Variables:\n\nApplying transformations like logarithmic, square root, or inverse can sometimes stabilize the variances.\n\nimport numpy as np\nlog_y = np.log(y)\nWeighted Least Squares (WLS):\n\nA variation of linear regression that weights each observation by the inverse of its variance.\n\nimport statsmodels.api as sm\nweights = 1 / np.var(X, axis=0)\nwls_model = sm.WLS(y, X, weights=weights).fit()\nGeneralized Least Squares (GLS):\n\nAn extension of WLS that allows for different correlations and variances among the observations.\n\ngls_model = sm.GLS(y, X).fit()\nRobust Standard Errors:\n\nAdjust the standard errors of the coefficients to account for heteroskedasticity without modifying the model itself.\n\nrobust_model = sm.OLS(y, X).fit(cov_type='HC3')\nHeteroskedasticity-Robust Regression:\n\nRegression models that are robust to heteroskedasticity.\n\nrobust_model = sm.RLM(y, X).fit()\nGeneralized Linear Models (GLM):\n\nA flexible generalization of ordinary linear regression that allows for response variables with error distribution models other than a normal distribution.\n\nimport statsmodels.api as sm\nglm_model = sm.GLM(y, X, family=sm.families.Gaussian()).fit()\nQuantile Regression:\n\nModels the relationship between variables for different quantiles (percentiles) of the data, which can be useful when heteroskedasticity is present.\n\nimport statsmodels.formula.api as smf\nquantile_model = smf.quantreg('y ~ X', data).fit(q=0.5)\nBreusch-Pagan Test or White Test:\n\nWhile not alternatives to linear regression, these tests can confirm the presence of heteroskedasticity, guiding you towards using one of the above methods.\n\n\nEach of these methods has its own assumptions and requirements, so it’s important to understand them fully before applying them to your data. The choice among them depends on the nature of your data and the specific research questions you are trying to answer.\n\n\nWhile a pretty good list, overall, I really dislike transformation being (1). Transformations (e.g. variance stabilizing transformations) are often presented as a first-choice solution, but applying them correctly is strongly problem dependent. They don’t only change the conditional variance, they change the functional relationship between the predictor and response, and as a result they can change the conditional distribution (i.e. the errors) is unexpected ways. Not to say that they aren’t useful, but they tend to be extremely problem dependent, in the sense that if you don’t know whether you should be transforming, you probably shouldn’t.\nSome of the other suggestions are a bit suspect. Quantile regression models, for instance, answer fundamentally different questions (i.e. conditional quantiles vs. the conditional mean), and the decision of which to use would generally be motivated by the research question rather than mere heteroskedasticity.\nImpressions:\n\nThe level of detail is about on par with most introductory statistics and data science articles, textbooks, or medium blog posts you can find on the internet. It seems to provide a reasonably thorough summary of the most common information on the internet, which sometimes means that its responses are inconsistent or incorrect (just like information on the internet).\nAs an example, it provides the very common and incorrect characterization of the Mann-Whitney as a test of medians. It also confuses the error and the residuals of a regression model, which is another common mistake in introductory articles and blog posts.\nIt is occasionally inconsistent or incoherent in ways that would be hard to spot for someone who wasn’t already knowledgeable about the material. Its description of the Mann-Whitney test is a good example: At a surface level, it’s a well written and thorough description, and it takes careful reading to realize that it contradicts itself several times, and gives an inconsistent description of the actual hypothesis being tested.\n\nVerdict: Technically, extremely impressive. Probably very useful for brainstorming analytical approaches for someone with a strong background. Probably a dangerous teacher for someone trying to learn."
  },
  {
    "objectID": "posts/2019-01-16-more-spurious-modular-structure/index.html",
    "href": "posts/2019-01-16-more-spurious-modular-structure/index.html",
    "title": "More spurious modular structure in functional brain networks",
    "section": "",
    "text": "Even in the absence of any true underlying network structure, functional brain networks can easily show complex, modular structure, possibly due to a combination of the transitive-ish properties of most functional connectivity measures, and the standard approaches to fMRI preprocessing (esp. low pass filtering)\nAssessment of dynamic functional connectivity usually involves estimating functional connectivity within a temporal sliding window, which requires the arbitrary selection of a window size. As with low-pass filtering, a small window length reduces the precision of the functional connectivity estimate by reducing the effective sample size, which should similarly result in greater modular structure.\nThis time, we’ll use real fMRI data consisting of an atlas of 175 regions recorded for a duration of 150 TRs. To ensure that there is no true underlying modular structure, I’ve randomly selected each of the 175 signals from different ROIs from different subjects during different scans (resting state vs. various tasks), and reversed half of the signals in time. We then construct correlation networks at window lengths of 10, 25, 50, 100, and 150 TRs by thresholding the sample correlation matrix at positive significant values using a t-test with an uncorrected threshold of \\(\\alpha = .05\\).\nAs we know, t-tests for correlations are wildly miscalibrated when applied to fMRI time series, and this problem is especially severe at short window lengths. Take the figure below, which shows a pair of signals (left) and the distribution of pairwise sample correlations (right) at each window length.\n\n\n\n\n\nNotice the increased prevalence of extreme correlations at smaller window lengths – the result of the near linearity of the signals within the window. For all the reasons discussed in the previous post, we would expect this to result in increased modular structure at shorter window lengths. Sure enough, short window lengths result in extremely modular, efficient, small-world networks, even in the absence of any real underlying network structure."
  },
  {
    "objectID": "posts/2024-01-11-quantum-brain-somethingorother/index.html",
    "href": "posts/2024-01-11-quantum-brain-somethingorother/index.html",
    "title": "Synergizing reinforcement learning with functional deep quantum neural manifold learning",
    "section": "",
    "text": "It’s been more than a few years since I’ve actually played around with any gambling tasks, and in the intervening time, models of sequential decision making based on concepts from quantum mechanics were briefly in vogue in several journals devoted to decision making and learning. I haven’t studied any quantum mechanics in a while, either, so I thought it would be interesting to unpack one of these models: Specifically, the model of Iowa Gambling Task performance reported by Li et al. (2020), which claims to find better fits to Human performance than classical models, such as ones based on the delta learning rule.\nI admit I’m largely ignorant of most of these models’ uses in the literature. There do seem to be cases where quantum and classical models produce measurably different predictions, and in where quantum models appear to explain patterns of preference judgement not captured by classical models (e.g. Epping and Busemeyer 2023); but the cynic in me wonders if a large part of the appeal isn’t the quantum language itself, especially as descriptions of these models in the literature tend to bury the reader in notation without providing insight.\nIn this case, the problem to solve is a standard Iowa gambling task (IGT), which is just a four-armed bandit disguised as a card game. On each trial, subjects are presented with four decks of cards, and upon choosing one, receives a monetary reward. The rewards associated with each deck are probabilistic, and the subject is told to earn as much money as possible by the end of the task, and must do so by selecting from each deck and learning which of the four is the most profitable.\nA standard model for this task looks something like this: The subject maintains a valuation \\(v = (v_1,v_2,v_3,v_4)\\), which encodes something like the expected utility for each deck. Upon choosing a card \\(i\\), the subject receives a payout \\(x\\), which has utility \\(u(x)\\). The subject then updates the valuation for deck \\(i\\) according to some update rule, and chooses another card with probability computed by passing \\(v\\) through a softmax function. Typically, the utility function and update rules depend on subject-specific parameters encoding e.g. learning rate, loss aversion, etc., and these parameters are estimated from subjects’ performance in order to draw inference about the subject’s decision making strategy (see e.g. Steingroever, Wetzels, and Wagenmakers 2013 for a taxonomy of similar models).\nNow, onto the quantum model. Let’s dispense with the quantum mechanical formalism, and especially with the quantum mechanical notation, which is needlessly opaque for anyone outside of the field. In finite dimensions (the only case anyone ever actually considers in these applications), nothing happens that can’t be more concisely written using standard matrix notation; and so we’ll treat this as an ordinary model of sequential decision making with a weird update rule, and we’ll use standard linear algebraic notation.\nAssume that the subject maintains a valuation \\(z(t) = (z_1,z_2,z_3,z_4)\\) for each deck on trial \\(t\\), where at this point the only notable departure is that \\(z\\) is a vector of complex numbers. After selecting deck \\(i\\), they observe an outcome with utility \\(u\\). The update rule is slightly complex, so we’ll proceed in steps. Ultimately, we’ll define a matrix \\(G\\) so that the valuation on the next trial is\n\\[\n  z(t+1) = Gz(t),\n\\] so that, fundamentally, we’re just working with a linear update.\nThe matrix \\(G\\) (which we’ll call the Grover matrix) is decomposed into a pair of matrices \\(G = U_2U_1\\) which depend, respectively, on the chosen deck and on the current valuation \\(z(t)\\). Letting \\(z\\) be the current valuation and letting \\(a\\) be an indicator vector for the chosen deck (i.e. if deck \\(i\\) was chosen, the \\(i\\)’th entry is 1, and the rest are zero), they are defined as follows:\n\\[\\begin{align*}\n  U_1 &= I - (1 - e^{i\\psi_1})aa^{\\top} \\\\\n  U_2 &= (1 - e^{i\\psi_2})zz^{\\top} - I\n\\end{align*}\\] where \\(I\\) is the identity matrix.\nTo be very explicit, for those uncomfortable working with complex numbers, the notation \\(e^{i\\psi_1}\\) denotes the complex number \\(cos(\\psi_1) + isin(\\psi_1)\\), and the transpose \\(z^\\top\\) always denotes the conjugate transpose (i.e. an entry \\(a+bi\\) becomes \\(a-bi\\)). To be even more explicit:\n\nComputeGrover &lt;- function(psi, a, z) {\n  ndecks &lt;- length(a)\n  I  &lt;- diag(complex(ndecks, real = 1, imaginary = 0))\n  e1 &lt;- 1 - complex(modulus = 1, argument = psi[1])\n  e2 &lt;- 1 - complex(modulus = 1, argument = psi[2])\n  \n  U1 &lt;- I - e1 * (a %*% t(Conj(a)))\n  U2 &lt;- e2 * (z %*% t(Conj(z))) - I\n  \n  return(U2 %*% U1)\n}\n\nThe only thing that remains is to define \\((\\psi_1,\\psi_2)\\). The authors define the quantities as a function of the utility \\(u\\):\n\\[\\begin{align*}\n  \\psi_1 &= \\pi (u \\cos(\\pi \\eta) + b_1) \\\\\n  \\psi_2 &= \\pi (u \\sin(\\pi \\eta) + b_2),\n\\end{align*}\\] where \\((\\eta, b_1, b_2)\\) are parameters (but see the discussion below). Again, to be explicit:\n\nComputePsi &lt;- function(u, pars) {\n  eta &lt;- pars$eta\n  b1  &lt;- pars$b1\n  b2  &lt;- pars$b2\n  psi &lt;- c(pi * (u * cos(pi*eta) + b1),\n           pi * (u * sin(pi*eta) + b2))\n  return(psi)\n}\n\nOnce we have the updated valuations \\(z\\), the probability of selecting deck \\(i\\) is the squared modulus \\(|z_i|^2\\).\n\nComputeChoiceProbs &lt;- function(z) {\n  Mod(z)^2\n}\n\nComplexity aside, what I want to make clear here is that, however the update rule was derived, ultimately we are just fitting curves to subjects’ behavior. In particular, the snippets of code that I’ve provided will implement this learning rule on your very own, boring, non-quantum computer. Using this model, your stupid classical laptop will succeed in learning a Bandit task like the IGT, even though it has never even seen Oppenheimer and nothing “quantum” is happening in the way your CPU manipulates bits in memory. This is, first and foremost, a flexible curve-fitting algorithm.\n\nIntuition\nConsider the more straightforward update of the delta learning rule, where the subject maintains a valuation \\(x = (x_1,x_2,x_3,x_4)\\) denoting the expected utility of each deck, and updates the valuation of the chosen deck \\(x_i\\) according to\n\\[\n  x_i(t+1) = x_i(t) + k[u - x_i(t)].\n\\]\nThat is, if the observed utility was higher than expected, we update our expectation upward; and vice versa. The actual mechanism underlying the update of our quantum learning rule is much more opaque, but we can get some geometric intuition.\nSuppose our current valuation of the four decks is \\(z(t) = (z_1,z_2,z_3,z_4)\\), and so the probability that we choose deck \\(i\\) is \\(|z_i|^2\\). This implies that \\(\\sum_{i=1}^4 |z_i|^2 = 1\\), and so \\(z\\) is a unit vector lying on a sphere. The norm \\(|z_i|\\) is just the projection of \\(z\\) onto the \\(i\\)-axis, and so the probability that we select this deck is roughly proportional to how much \\(z\\) points in the direction of the corresponding deck.\nThis is easiest to see in the simple two-dimensional example in the figure below, where we imagine two decks \\(x\\) and \\(y\\), and a (real-valued) valuation \\(z = (z_1,z_2)\\). Here, the (square of the norm of the) projections of \\(z\\) onto the \\(x\\) and \\(y\\)-axes determine the probabilities of the respective deck selections.\n\n\n\n\n\n\n\n\n\nNote that, however we update \\(z\\) after observing the outcome of our selection, the new valuation \\(z'\\) must also lie on the sphere, and so can be obtained by applying a rotation to \\(z\\) about the origin. Sensibly, we would like to rotate \\(z\\) to point “more towards” the chosen deck if it payed well. Intuitively, this is a perfectly sensible learning rule.\nThis is, in fact, precisely what is happening. The matrices \\(U_1\\) and \\(U_2\\) are unitary transformations, and so in particular are simply rotations (and reflections) about the origin. Assuming our axes\n\\[\\begin{align*}\n  a_1 &= (1,0,0,0) \\\\\n  a_2 &= (0,1,0,0) \\\\\n  a_3 &= (0,0,1,0) \\\\\n  a_4 &= (0,0,0,1)\n\\end{align*}\\]\ncorrespond to the four respective decks, we would like the result of applying \\(G = U_2U_1\\) to be a rotation/reflection of \\(z\\) to point more parallel to or more orthogonal to the chosen deck, depending on whether the outcome was better or worse than expected, respectively.\nThrough a rather elaborate derivation, the authors give the ratio \\(R\\) of this change in probability (that is, the ratio of the probability of deck \\(a\\) before and after having observed the outcome with utility \\(u\\)):\n\\[\n  R = \\left |(1 - e^{i\\psi_1} - e^{i\\psi_2}) - (1 - e^{i\\psi_1})(1 - e^{i\\psi_2})|z_a|^2 \\right |^2\n\\] This closed-form update is especially useful, as it let’s us directly visualize some unusual model behavior.\n\n\nWhat does the update look like?\nNow, here’s the point of this exercise. We’ll consider two models in parallel, which we’ll call QM (for quantum model) and CM (classical model). Both will use the same utility, but QM will apply the update we’ve just discussed, while CM will use the delta learning rule along with a softmax function to determine deck selection probabilities.\n\n\n\n\n\n\n\n\n\nCM\nQM\n\n\n\n\nValuation\n\\(\\mathbb{R}\\)-valued\n\\(\\mathbb{C}\\)-valued\n\n\nUpdate\n\\(z_i(t+1) = z_i(t) + r[u - z_i(t)]\\)\n\\(Gz(t)\\)\n\n\nSelection\n\\(P(k) = \\frac{e^{z_k}}{\\sum_{i=1}^4 e^{z_i}}\\)\n\\(P(k) = |z_k|^2\\)\n\n\nInitial valuations\n\\(z_k = 0\\)\n\\(z_k = \\frac{1}{2} + 0i\\)\n\n\n\nThis is what we’re going to do: We’re going to pick deck (say, deck 1) and look at the change in selection probability for that deck as a function of the utility. Specifically, since we have a closed-form expression for this for the QM model, we’ll be comparing the log-odds of selecting the deck before and after the outcome, as a function of the model parameters. Here is it for the model CM:\n\n\n\n\n\n\n\n\n\nNote that this is completely sensible. As the utility increases, our probability of selecting the deck increases as the utility of the outcome increases, and this effect is greater for higher learning rates \\(r\\).\nNow onto the, uh…difficulty of model QM. First things first, we’ll ignore the parametrization in terms of \\(u\\), and just focus on the log-odds of selecting the chosen deck as a function of \\((\\psi_1,\\psi_2)\\), however they are calculated. Note that, as these are angles, they are periodic with period \\(2\\pi\\), and so it suffices to consider only this range.\n\n\n\n\n\n\n\n\n\nFor clarity, the red region corresponds to values of \\((\\psi_1,\\psi_2)\\) leading to an increased probability of selecting the chosen deck, and vice versa for the blue. What we want to do is define some continuous function \\(f\\) which maps the observed utility (maybe depending on some parameters) to some value of \\((\\psi_1,\\psi_2)\\). And we would like to do this in a way that we end up in the red region if the utility is positive, and in the blue region otherwise. Note that we specify continuous because would expect similar utilities to result in similar updates. If this function has parameters, we would also like them to be interpretable, in the way that \\(r\\) in the delta learning rule can clearly be interpreted as the learning rate.\nAs the authors point out, the mapping\n\\[\\begin{align*}\n  \\psi_1(u; \\eta, b_1, b_2) &= \\pi (u \\cos(\\pi \\eta) + b_1) \\\\\n  \\psi_2(u; \\eta, b_1, b_2) &= \\pi (u \\sin(\\pi \\eta) + b_2),\n\\end{align*}\\]\ndefines a line passing through the point \\((b_1,b_2)\\), with slope \\(\\tan(\\pi \\eta)\\). Intuitively, the mapping \\(G\\) defines a surface mapping the \\((\\psi_1,\\psi_2)\\) to the change in deck selection probability, and \\((\\psi_1,\\psi_2)_{\\eta,b_1,b_2}\\) maps the observed utility to a one-dimension slice of this space, as depicted below. Note that, despite what the plot below indicates (as I don’t have the energy to correct it), the “center” of this line (corresponding to zero utility) is at the point \\((\\pi b_1, \\pi b_2)\\).\n\n\n\n\n\n\n\n\n\nThere are two things I dislike about this parametrization as a model for Human behavior. First, the parameters are extremely difficult to interpret, as the relationship between utility and the change in deck probability are very difficult to read from the parameter values themselves. If the user knows the landscape of the function, they can compute the effect of zero utility by looking at whether the update implied by \\((\\pi b_1, \\pi b_2)\\) is positive or negative, but the effect of \\(\\eta\\) in particular is entirely dependent on the basepoint \\((b_1,b_2)\\), and can imply opposite things about the effect of increasing utility depending on the specific values.\nThe second is that, because the log-odds are periodic in \\((\\psi_1,\\psi_2)\\), any unbounded function of \\(u\\) (such as this one) will exhibit oscillatory behavior, as shown in the example below:\n\n\n\n\n\n\n\n\n\nWe can see this exactly by choosing some specific parameters – say, \\((\\eta, b_1, b_2) = (0.1, .9, .7)\\). Then, here are the log-odds for different utilities:\n\n\n\n\n\n\n\n\n\nActually, we can be more specific. As the update is periodic in both \\(\\psi_1\\) and \\(\\psi_1\\), the parameter space of \\(G\\) is actually a torus (seen below).\n\n\n\n\n\n\n\n\n\nThe parametrization of the update in terms of (\\(\\eta, b_1, b_2\\)) means that the change in log-odds is the orbit of a linear flow, which immediately suggests some odd behavior. For one, for any rational \\(\\eta\\), the orbit is a closed curve, and so the update is periodic in the observed utility. This means that the model cannot increase its preference for a deck monotonically with the increasing utility: If a small negative utility results in a decrease in deck preference, then a large enough positive utility must also result in a decrease in preference. I’m not sure if it’s realistic to expect that people learn this way. In fact, the global model behavior seem obviously inconsistent with the way that people actually learn.\nI suspect that, for a specific payoff schedule, it would be possible to carefully tailor the parameters so that the log-odds over the range of experienced utility lies in a well behaved region of the space. For example, if the utility experienced by the subject is constrained within \\((-.2, .5)\\), then the parameters \\((\\eta, b_1, b_2) = (0.1, 0, .5)\\) give a reasonable update:\n\n\n\n\n\n\n\n\n\nFor these specific values of \\((b_1, b_2)\\), the parameter \\(\\eta\\) is almost interpretable as something like an inverse learning rate:\n\n\n\n\n\n\n\n\n\nIn fact, we get a pretty close approximation of the curves generated by the CM for different learning rate parameters. But not so for slightly different values \\((b_1, b_2) = (-.2, .3)\\):\n\n\n\n\n\n\n\n\n\nNow, \\(\\eta\\) is hardly interpretable at all. For positive values, all utilities near zero result in a decreased selection probability, while negative values gives an almost-sensible but non-monotonic change in probability. In this sense, the model is extremely flexible for a relatively small number of parameters (only one additional parameter compared to something like the PVL-delta), at the cost of the parameters being nearly uninterpretable from a cognitive or behavioral standpoint, arguably negating the primary use of these models in psychology and neuroscience.\nWhen we dispense with the quantum language and view it merely as a model of sequential decision making with a particularly flexible policy update, the structure of the model is quite simple: We have a smooth surface – a function of \\((\\psi_1,\\psi_2)\\) – and we take a slice of this surface by defining a line in the plane (a function of \\(\\eta\\), \\(b_1\\), and \\(b_2\\)). This slice gives the change in log-odds of selection as a function of the observed utility. Since the surface is reasonably complex, we can get a wide variety of these probability curves depending on where we place the line; hence the flexibility of the model.\nAs a model, it’s interesting, but since it is fundamentally just curve fitting, and since the weird oscillatory learning behavior is clearly inconsistent with the way decision making is likely to be implemented in the brain, I’m not sure that the model’s performance really suggests that the brain is leveraging “quantum” phenomena. Shor’s algorithm is a polynomial time quantum algorithm for integer factorization. And yet, we wouldn’t claim that the ability of a subject to factor small numbers in their head is evidence for some kind of quantum computation in the brain. Classical algorithms can do prime factorization too. Quantum algorithms are designed to take advantage of the architecture of a quantum computer, but they typically solve the same problems. The mere fact that a model based on quantum computation can predict behavior (e.g. the output of some computation) does not, by itself, suggest any kind of quantum computation in the brain unless we can also show that something in the computational architecture of the brain is actually quantized.\n\n\n\n\n\nReferences\n\nEpping, Gunnar P, and Jerome R Busemeyer. 2023. “Using Diverging Predictions from Classical and Quantum Models to Dissociate Between Categorization Systems.” Journal of Mathematical Psychology 112: 102738.\n\n\nLi, Ji-An, Daoyi Dong, Zhengde Wei, Ying Liu, Yu Pan, Franco Nori, and Xiaochu Zhang. 2020. “Quantum Reinforcement Learning During Human Decision-Making.” Nature Human Behaviour 4 (3): 294–307.\n\n\nSteingroever, Helen, Ruud Wetzels, and Eric-Jan Wagenmakers. 2013. “A Comparison of Reinforcement Learning Models for the Iowa Gambling Task Using Parameter Space Partitioning.” The Journal of Problem Solving 5 (2): 2."
  },
  {
    "objectID": "posts/2018-12-10-spurious-modular-structure/index.html",
    "href": "posts/2018-12-10-spurious-modular-structure/index.html",
    "title": "Spurious modular structure in functional brain networks",
    "section": "",
    "text": "EDIT (Feb 26, 2019): So it turns out I’m not the first to notice this. Jaroslav Hlinka et al. identified this problem as early as 2012. They even attribute it to the same cause: the (partial) transitivity of the sample correlation.\n\nThe small-world properties of fMRI functional connectivity graphs obtained using standard methods have been shown to be largely reproduced or even exceeded by a matching randomly connected multivariate autoregressive process. This result shows for the first time that the small-world properties of functional connectivity real-world graphs can be indeed attributed to the transitive properties of the correlation coefficient, as previously conjectured.\n\nThey go on to conclude\n\nOur results suggest that most, if not all, of the observed effect in the brain data is attributable to the small-world bias of the correlation matrix\n\nIt’s certainly disappointing that no one in the network neuroscience community has felt the need to acknowledge this fact, even though several of them have published in this same journal.\n\nHlinka, J., Hartman, D., & Paluš, M. (2012). Small-world topology of functional connectivity in randomly connected dynamical systems. Chaos: An Interdisciplinary Journal of Nonlinear Science, 22(3), 033107.\n\n\nHlinka, J., Hartman, D., Jajcay, N., Tomeček, D., Tintěra, J., & Paluš, M. (2017). Small-world bias of correlation networks: From brain to climate. Chaos: An Interdisciplinary Journal of Nonlinear Science, 27(3), 035812.\n\n\nOriginal post:\nMehler and Kording (2018) just put out a very nice paper on the very flimsy theoretical foundations of network neuroscience, the general impossibility of inferring causal dynamics, or even connectivity itself, from standard approaches to brain connectivity analysis, and the inability to accurately recover the underlying network structure. Riffing on this idea, I want to point out one specific example where properties of networks constructed from neuroimaging data don’t necessarily reflect any real property of the underlying brain circuitry.\nGraph theoretic approaches to the analysis of functional brain connectivity use recorded brain activity (most commonly, BOLD fMRI) during resting state or task to construct graphs representing (supposedly) functional connectivity between distinct brain regions. These graphs are often claimed to possess some kind of modular structure – roughly, a clustered organization in which nodes are densely connected within clusters, and relatively sparsely connected between them. This is usually considered to be a special or noteworthy feature of biological networks, as “random graphs” don’t generally display this kind of modularity. More importantly, the modularity observed in functional brain networks is usually assumed to reflect some underlying organization or structure within the brain – that is, the organization of the brain into distinct functional modules. It’s this second view that I take issue with. Below, I’ll show how the techniques used to estimate functional brain networks can easily engineer highly modular networks, even when all nodes in the network are completely independent. It follows that the mere appearance of modularity in empirical brain networks is not, in and of itself, evidence for any particular modular organization within the brain.\nSuppose that we record mean BOLD signals from a set of brain regions during the performance of a cognitive task. We then apply a standard fMRI preprocessing pipeline, band-pass filter the data within a frequency range commonly associated with cognitive performance, compute the pairwise correlations between regions, and then threshold the correlation matrix to obtain a graph. We compute a few summary statistics for the graph and find that it has substantial structure compared to a null model – a random graph in which vertices are joined by an edge independently at random. Do we then have evidence that these brain regions are communicating with each other, or exhibit some kind of large scale network structure?\nHere’s the tricky part: What we meant by our null model is a network in which there are no true relationships between brain regions. What we actually specified was a random graph. The problem is that the two are not the same – standard approaches to preprocessing fMRI data and constructing functional networks can introduce spurious structure into the graph, even when the underlying signals are independent.\nLet’s simulate some data: We’ll generate 250 datasets consisting of \\(N = 150\\) white noise signals of length \\(T = 100\\). FMRI data are typically low pass filtered, and functional connectivity analyses are usually carried out in frequency ranges roughly between .025 Hz and .15 Hz, so we’ll low-pass filter the data with a cutoff between .025 Hz and .175 Hz, in steps of .025. For each of these cutoffs, we’ll construct graphs by thresholding the matrices of pairwise correlations between signals.\nWhat kind of structure can we expect to see, keeping in mind that our signals are pure, uncorrelated white noise? To start with, let’s get a sense of what the correlations actually look like. For each filter cutoff, I’ve plotted the distribution of the Pearson correlation coefficient below:\n\n\n\n\n\nNotice the increased variance and, ultimately, clustering at the boundaries, at lower frequencies. This is completely intuitive: consider the limiting case, when the smoothing is so severe that the signals are essentially linear over their recorded length. In that case, any two either increase/decrease together (a perfect correlation); one increases while another decreases (perfect anti-correlation); or one or both of the signals are zero (zero correlation). So the correlation is effectively binarized in the limit.\nAs a digression, this has practical consequences for researchers who misuse significance tests. What is a “significant” correlation? Using a standard t-test, the test statistic for a correlation \\(r\\) is [ t = r ] which has a \\(t_{n-2}\\) distribution under the null hypothesis of zero correlation under the assumption that the \\(n\\) observations are independent and bivariate normal. But smoothing (or low-pass filtering in general) introduces autocorrelation, which makes larger correlations more common, which inflates the false positive rate of the test. See below for the error rate for a significance threshold of \\(\\alpha = .05\\):\n\n\n\n\n\nSo for a filter cutoff of .175 Hz (in the ballpark of commonly used cutoffs in the fMRI literature), a t-test with a nominal type I error rate of .05 has an actual error rate of over .2 – over four times higher. See Lazic (2010) for other examples of scientists not understanding dependence.\nThis clustering towards the boundaries actually has interesting implications for the structure of thresholded correlation networks. Now, correlation is not transitive in general – if \\(X\\) and \\(Y\\) are positively correlated, and \\(Y\\) and \\(Z\\) are positively correlated, it is not necessarily the case that \\(X\\) and \\(Z\\) are also positively correlated. But there’s a limit – because covariance is an inner-product, a little linear algebra gives us [ r_{XZ} r_{XY}r_{YZ} - . ] If both \\(r_{XY}\\) and \\(r_{YZ}\\) are at least \\(\\theta\\), then this implies \\(r_{XZ} \\geq 2\\theta^2-1\\). In other words, sufficiently extreme correlations exhibit transitivity. What does transitivity mean in the context of a graph? It means triangles. So two things happen here: The clustering of the correlation coefficient towards the boundaries at lower filter cutoffs increases the prevalence of triangles in the resulting correlation network, and the process of thresholding – removing lower correlations – selectively kills those edges least likely to be part of a triangle. We can see this through simulation: here are the mean number of triangles for the correlation networks constructed for each filter cutoff (thresholded at 30% edge density, as advocated by Zhang, et al., 2016).\n\n\n\n\n\nAt the most extreme cutoff, we have over twice as many triangles  for the same edge density. What does this mean for the graph topology? It means clustering. We can take a look at a few measures: Louvain modularity, the clustering coefficient, Humphries’ (2008) measure of small worldness, and the number of clusters (as detected by Louvain clustering)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo we see that, at low frequencies, correlation networks constructed from uncorrelated white noise exhibit: A) substantial modularity; B) substantial clustering; C) “small-worldness”; and D) a relatively small number of clusters. In other words, they look exactly like functional brain networks.\nOf course, “substantial modularity” (for example) is only substantial relative to a null model – commonly chosen to be something like an Erdos-Renyi random graph, in which pairs of vertices are joined independently at random, or a Newman–Girvan null model, which preserves the expected degree of each node Both of these null models are arguably scientifically uninteresting, as when edges represent correlations (or other measures which behave like inner-products; i.e. kernels), edges cannot be added or removed independently due to the correlation’s (partial) transitivity. This means that correlation graphs constructed from random noise are not random graphs in the Erdos-Renyi or Newman-Girvan sense – they have more structure. Tests against these null models don’t necessarily provide the information we want, which is whether an observed correlation network exhibits modularity beyond the substantial modularity already intrinsic to correlation networks constructed from autocorrelated time series,\nOf course, none of this is to say that all reported functional connectivity analyses are spurious – many authors have successfully used functional connectivity estimates to do prediction, for example – but published claims that fMRI correlation networks exhibit “modular structure” are much less impressive when similar degrees of modularity can obtained from filtered white noise.\n\nReferences\n\n\nHumphries, M. D., & Gurney, K. (2008). Network ‘small-world-ness’: a quantitative method for determining canonical network equivalence. PloS one, 3(4), e0002051.\n\n\nLazic, S. E. (2010). The problem of pseudoreplication in neuroscientific studies: is it affecting your analysis?. BMC neuroscience, 11(1), 5.\n\n\nMehler, D. M. A., & Kording, K. P. (2018). The lure of causal statements: Rampant mis-inference of causality in estimated connectivity. arXiv preprint arXiv:1812.03363.\n\n\nZhang, Z., Telesford, Q. K., Giusti, C., Lim, K. O., & Bassett, D. S. (2016). Choosing wavelet methods, filters, and lengths for functional brain network construction. PloS one, 11(6), e0157243."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Cognitive contributions to motor control\n\n\n\n\n\nWhen acting in an uncertain environment, or when the action needed to achieve a goal is unknown, cognitive structures in the brain are engaged to analyze and decipher the problem’s structure, and dictate the parameters that the motor system uses to program the needed action. These processes are especially important in the early stages of learning, before the appropriate action has been decided upon, and the motor system can take over the process of adaptation. Most of my work uses neuroimaging methods to study functional interactions between motor and non-motor structures during this early stage of learning.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMethods for analyzing fMRI functional connectivity\n\n\n\n\n\nThe analysis of fMRI functional connectivity typically involves the analysis of very high-dimensional covariance matrices, encoding the BOLD signal covariance between brain regions or voxels. The most popular of these approaches, which I’ve used extensively in my imaging work, takes advantage of the Riemannian-manifold structure of the space of covariance matrices.\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "packages/MSMVSampEn.html",
    "href": "packages/MSMVSampEn.html",
    "title": "MSMVSampEn",
    "section": "",
    "text": "MSMVSampEn is an R package for computing multiscale, multivariate sample entropy using the procedure outlined by Ahmed and Mandic (2011).\n\nInstallation\nMSMVSampEn is designed for efficiently computing multiscale sample entropy from highly multivariate biological signals (e.g. eeg, meg, and fMRI BOLD data). The package can be installed directly from its Github repository using\ndevtools::install_git('https://github.com/areshenk-rpackages/MSMVSampEn')\n\n\nIssues\nNote that the interface of MSMVSampEn is likely to undergo substantial changes in the near future in order to accommodate additional features. As it stands, the package consists largely of a single wrapper function implementing the procedure of Ahmed and Mandic (2011). That said, there are a few issues to be aware of:\n\nThe implementation of the MSMV sample entropy itself has been tested and validated (by e.g. Chen et al. 2019), but as the authors note, it does not handle very large datasets (on the order of tens of thousands of timepoints) particularly well. This is almost certainly a memory issue, and I’m in the process of hunting it down; but as this is rarely a problem in my own use cases, it is not a high priority.\nThe computation of the sample entropy at longer timescales is accomplished by computing means across a sliding window, which is now recognized as being, in effect, a crude low-pass filter with poor properties. More modern approaches allow for more stable entropy estimates at shorter signal lengths, but these have not yet been implemented.\n\n\n\nReferences\nAhmed, M. U., & Mandic, D. P. (2011). Multivariate multiscale entropy: A tool for complexity analysis of multichannel data. Physical Review E, 84(6), 061918.\nChen, C., Sun, S., Cao, Z., Shi, Y., Sun, B., & Zhang, X. D. (2019). A comprehensive comparison and overview of R packages for calculating sample entropy. Biology Methods and Protocols, 4(1), bpz016."
  },
  {
    "objectID": "packages/spdm.html",
    "href": "packages/spdm.html",
    "title": "spdm",
    "section": "",
    "text": "spdm is an R package implementing operations on symmetric, positive-definite matrices (aka covariance matrices). Besides offering several forms of regularized covariance estimation, spdm is focused on geometry-respecting covariance analysis – that is, operations which respect the intrinsic Riemannian manifold structure of the set of the symmetric positive-definite matrices. In my own work, these techniques are usually applied to the analysis of fMRI functional connectivity.\n\nInstallation\nspdm can be installed from GitHub with using install_git() from the devtools package using the command below:\ndevtools::install_git('areshenk-rpackages/spdm')\n\n\nIssues\nspdm is diligently tested, and is under ongoing development. That said, the package comes equipped with two main caveats:\n\nMany of the operations implemented in spdm involve the manipulation (commonly, inversion) of large matrices. The sample covariance is often ill-conditioned – particularly when the sample size is not considerably larger than the dimension – and inversion can result in the accumulation of large amounts of error. This can be partially mitigated through the use of well-conditioned estimators (such as the Ledoit-Wolf shrinkage estimator), which should almost always be chosen over the sample covariance (for many reasons, numerical and statistical). However, I am not a specialist in numerical linear algebra, and I make no guarantees about the error of any given operation. I am slowly planning more sanity tests to flag these kinds of numerical issues, but have not done so yet.\nA major use for spdm in my own work is in the centering and subject alignment of functional connectivity data via parallel transport. This works wonderfully and is, I’m convinced, an essential part of any analysis of task differences in functional connectivity. There is, however, an interpretational issue, in that the resulting (centered) covariance matrices are distorted by the centering procedure, and the entries correspond to linear combinations of uncentered covariances. This issue is more severe when transporting over larger distances (where the curvature of the space becomes significant), but as of yet there are no formal procedures for actually characterizing this distortion."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Preprints\n\nAreshenkoff, C. N., Brouwer, A. J. de, Gale, D. J., Nashed, J. Y., Randall Flanagan, J., Smallwood, J., & Gallivan, J. P. (2023). Distinct patterns of connectivity with motor cortex reflect component processes of sensorimotor learning. bioRxiv, 2023–2007.\n\n\n\nPublications\n\nBrien, D. C., Riek, H. C., Yep, R., Huang, J., Coe, B., Areshenkoff, C. N., Grimes, D., Jog, M., Lang, A., Marras, C., et al. (2023). Classification and staging of parkinson’s disease using video-based eye tracking. Parkinsonism & Related Disorders, 105316.\nStandage, D. I., Areshenkoff, C. N., Gale, D. J., Nashed, J. Y., Flanagan, J. R., & Gallivan, J. P. (2023). Whole-brain dynamics of human sensorimotor adaptation. Cerebral Cortex, 33(8), 4761–4778.\nGale, D. J., Areshenkoff, C. N., Standage, D. I., Nashed, J. Y., Markello, R. D., Flanagan, J. R., Smallwood, J., & Gallivan, J. P. (2022). Distinct patterns of cortical manifold expansion and contraction underlie human sensorimotor adaptation. Proceedings of the National Academy of Sciences, 119(52), e2209960119.\nAreshenkoff, C. N., Gale, D. J., Standage, D., Nashed, J. Y., Flanagan, J. R., & Gallivan, J. P. (2022). Neural excursions from manifold structure explain patterns of learning during human sensorimotor adaptation. Elife, 11, e74591.\nBrouwer, A. J. de, Areshenkoff, C. N., Rashid, M. R., Flanagan, J. R., Poppenk, J., & Gallivan, J. P. (2021). Human variation in error-based and reinforcement motor learning is associated with entorhinal volume. Cerebral Cortex.\nAreshenkoff, C. N., Nashed, J. Y., Hutchison, R. M., Hutchison, M., Levy, R., Cook, D. J., Menon, R. S., Everling, S., & Gallivan, J. P. (2021). Muting, not fragmentation, of functional brain networks under general anesthesia. Neuroimage.\nGale, D. J., Areshenkoff, C. N., Honda, C., Johnsrude, I. S., Flanagan, J. R., & Gallivan, J. P. (2021). Motor planning modulates neural activity patterns in early human auditory cortex. Cerebral Cortex.\nStandage, D., Areshenkoff, C. N., Nashed, J. Y., Hutchison, R. M., Hutchison, M., Heinke, D., Menon, R. S., Everling, S., & Gallivan, J. P. (2020). Dynamic reconfiguration, fragmentation and integration of whole-brain modular structure across depths of unconsciousness. Cerebral Cortex.\nKarr, J. E., Areshenkoff, C. N., Rast, P., Hofer, S. M., Iverson, G. L., & Garcia-Barrera, M. A. (2018). The unity and diversity of executive functions: A systematic review and re-analysis of latent variable studies. Psychological Bulletin, 144(11), 1147.\nTrafimow, D., Amrhein, V., Areshenkoff, C. N., Barrera-Causil, C. J., Beh, E. J., Bilgiç, Y. K., Bono, R., Bradley, M. T., Briggs, W. M., Cepeda-Freyre, H. A., et al. (2018). Manipulating the alpha level cannot cure significance testing. Frontiers in Psychology, 9, 699.\nAreshenkoff, C. N., Bub, D. N., & Masson, M. E. (2017). Task-dependent motor representations evoked by spatial words: Implications for embodied accounts of word meaning. Journal of Memory and Language, 92, 158–169.\nSmart, C. M., Karr, J. E., Areshenkoff, C. N., Rabin, L. A., Hudon, C., Gates, N., Ali, J. I., Arenaza-Urquijo, E. M., Buckley, R. F., Chetelat, G., et al. (2017). Non-pharmacologic interventions for older adults with subjective cognitive decline: Systematic review, meta-analysis, and preliminary recommendations. Neuropsychology Review, 27(3), 245–257.\nKarr, J. E., Garcia-Barrera, M. A., & Areshenkoff, C. N. (2014). Executive functions and intraindividual variability following concussion. Journal of Clinical and Experimental Neuropsychology, 36(1), 15–31.\nKarr, J. E., Areshenkoff, C. N., Duggan, E. C., & Garcia-Barrera, M. A. (2014). Blast-related mild traumatic brain injury: A bayesian random-effects meta-analysis on the cognitive outcomes of concussion among military personnel. Neuropsychology Review, 24(4), 428–444.\nKarr, J. E., Areshenkoff, C. N., & Garcia-Barrera, M. A. (2014). The neuropsychological outcomes of concussion: A systematic review of meta-analyses on the cognitive sequelae of mild traumatic brain injury. Neuropsychology, 28(3), 321.\nKarr, J. E., Areshenkoff, C. N., Rast, P., & Garcia-Barrera, M. A. (2014). An empirical comparison of the therapeutic benefits of physical exercise and cognitive training on the executive functions of older adults: A meta-analysis of controlled trials. Neuropsychology, 28(6), 829.\n\n\n\nPresentations\n\nAreshenkoff, C. N., Brouwer, A. J. de, Gale, D. J., Nashed, J. Y., Flanagan, J. R., & Gallivan, J. P. (2023). The structural-functional neural architectures of implicit and explicit motor learning. Poster presented at the annual meeting for the Society for Neuroscience, Washington, DC.\nAreshenkoff, C. N., Gale, D. J., Nashed, J. Y., Standage, D., Flanagan, J. R., & Gallivan, J. P. (2021). Neural excursions from low-dimensional manifold structure in cognitive and sensorimotor brain networks explains intersubject variation in human motor learning. Paper presented at the annual meeting for the Society for the Neural Control of Movement.\nAreshenkoff, C. N., Nashed, J. Y., Hutchison, R. M., Hutchison, M., Menon, R. S., Everling, S., & Gallivan, J. P. (2020). Predicting depth of sedation from latent structure in whole-brain cortical networks. Paper presented at the annual meeting for the Organization for Human Brain Mapping.\nAreshenkoff, C. N., Brouwer, A. de, Nashed, J. Y., Gale, D., & Gallivan, J. P. (2019). Network-level interactions during sensorimotor adaptation learning and generalization. Poster presented at the annual meeting for the Society for Neuroscience, San Diego, CA.\nAreshenkoff, C. N., Nashed, J. Y., Standage, D., & Gallivan, J. P. (2018). Functional coupling between the basal ganglia and cerebellum during visuomotor adaptation learning. Poster presented at the annual meeting for the Society for Neuroscience, San Diego, CA.\nStandage, D., Nashed, J. Y., Areshenkoff, C. N., Flanagan, R. J., & Gallivan, J. P. (2018). Whole-brain modular structure of spontaneous neural activity at rest predicts future sensori-motor learning. Poster presented at the annual meeting for the Society for Neuroscience, San Diego, CA.\nAreshenkoff, C. N., Bub, D. N., & Masson, M. E. (2016). Parallel encoding of alternative action possibilities alters the execution of a cued action. Poster presented at the 31st meeting of the Society for the Neuroscience of Decision Making. Montreal, QC.\nAreshenkoff, C. N., & Bub, D. N. (2016). The temporal dynamics of motor affordances. Poster presented at the 31st meeting of the Society for the Neuroscience of Decision Making. Montreal, QC.\nMulligan, B. P., Areshenkoff, C. N., & Smart, C. M. (2015). EEG entropy predicts intensively measured cognitive performance in healthy older adults. Psychophysiology, 52, S25–S25.\nAreshenkoff, C. N., Bub, D. N., & Masson, M. E. (2015). Embodied representation of word meaning. Canadian Journal of Experimental Psychology-Revue Canadienne de Psychologie Experimentale, 69, 343–343.\nAreshenkoff, C. N., Karr, J. E., & Garcia-Barrera, M. (2013). Performance patterns of card selection in the iowa gambling task: Preliminary evidence of high sensitivity to losses following mTBI [abstract]. Journal of the International Neuropsychological Society, 19, 75.\nAreshenkoff, C. N., Karr, J. E., Crevier-Quintin, E., & Garcia-Barrera, M. (2013). The benefits of physical and cognitive training programs of executive functions among older adults: A meta-analytical comparison. Poster presented at the 41st annual meeting of the International Neuropsychological Society. Waikaloa, Hawaii.\nKarr, J. E., Areshenkoff, C. N., Evans, J., Jewett, K., David Nguyen, Tutt, G., & Garcia-Barrera, M. (2012). Physical exercise versus cognitive training on executive functioning: It’s a tie! A meta-analysis of controlled trials during healthy aging. Lecture presented at the annual meeting of the Canadian Association of Gerontology. Vancouver, BC.\nGrindstaff, T. R., Karr, J. E., Areshenkoff, C. N., Saville, K. A., Alexander, J. E., & Alexander, R. G. (2012). Frontal and parietal differences in P300 amplitude and latency during a self-evaluation task. Poster presented at the 24th annual convention of the Association for Psychological Science. Chicago, Illinois."
  },
  {
    "objectID": "packages.html",
    "href": "packages.html",
    "title": "Software",
    "section": "",
    "text": "MSMVSampEn\n\n\nMultiscale, multivariate sample entropy in R\n\n\n\n\n\n\n\n\n\n\n\n\n\niowa\n\n\nModular RL models of the Iowa gambling task\n\n\n\n\n\n\n\n\n\n\n\n\n\nspdm\n\n\nSymmetric, positive-definite matrices in R\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2018-02-18-cargo-cult-statistics/index.html",
    "href": "posts/2018-02-18-cargo-cult-statistics/index.html",
    "title": "Cargo cult statistics",
    "section": "",
    "text": "The failure of scientists, and particularly students in the sciences, to properly understand the most commonly used statistical concepts in their field has been extensively documented (see e.g. Sotos et al., 2007). Many of the specific misunderstandings are well known at this point to anyone with an interest in methodology – for example, Haller, et al. (2002) questioned students and faculty at several Universities and found that the overwhelming majority could not correctly define a p-value, a crisis that has launched a thousand publications.\nI’m not interested in these misunderstandings, because I don’t consider them to be very important. The failure of students and researchers to understand (say) basic hypothesis testing has led to a small cottage industry and several tenured careers spent attempting either to correct the specific misunderstanding, or to take away null hypothesis testing altogether and replace it with something like a Bayes factor. All of these efforts are doomed to fail because they ignore the fundamental problem: these misunderstandings are not isolated gaps in knowledge; they represent a fundamental lack of understanding of basic concepts in statistics and probability.\nIf we want to understand how these misunderstandings come about, thinking about how students learn statistics can be instructive.\nFor those who might be unfamiliar with the way that statistics is taught in the natural or behavioural sciences: undergraduate students typically take a department mandated course (or two) in data analysis sometime in their second or third year. Unless their major is highly quantitative, most students do not have a strong background in mathematics. Many will not have taken any math since high school, while others may have taken a required course in “Calculus for non-math majors”. As a result, the course is largely taught by pictures and analogy. Generally, instructors will comfort their students on the first day of class by reassuring them that, although there will be “formulas”, the main goal is for them to understand the “meaning” of the equations, and so gain some form of “intuition” about how to use statistics.\nGenerally, intuition is something I imagine resulting from a deep understanding of a field or concept, so that a person understands fundamentally how it behaves. How a student can gain an intuition for statistics without solving any statistical problems, or grappling with the central theorems of the field, is beyond my guessing. Intuition comes after technical proficiency, it’s not a replacement for it. This doesn’t mean that every student needs a background in measure theoretic probability, but shouldn’t a researcher who is going to work with statistical models as part of their career compute the expected value of a distribution at least once in their life? I’ve floated this idea by colleagues before, and most of them are strongly opposed to it. They generally respond with some variant of “I don’t need to know the math behind everything, I know it well enough to use it in practice”. I’ve even heard colleagues claim that they don’t think students should take courses in the statistics or mathematics departments because “those courses don’t teach them how to use statistics!”. And yet, here we are. Over 90% of psychology undergraduates cannot correctly interpret a hypothesis test. Apparently they didn’t get much intuition after all.\nSome courses may provide an introduction to probability – necessarily discrete, since the instructor can’t assume a background in calculus. This will consist mostly of when you should multiply or add probabilities, and a few examples of Bayes theorem. None of these will ever be used or discussed again, as almost all statistical models used in practice are continuous.\nThe course then moves on to common summary statistics. This is generally taught by giving formulas for the mean, median, standard deviation, and variance, and explaining that the median should be used if you plot a histogram and it looks asymmetrical. A typical exam will here present a skewed histogram with the question “Which measure of central tendency would you compute from this data, the mean or the median?” This is exactly the kind of question that encourages the mindless parroting of trivia. Sure, the mean will be influenced by the skew in the data, but the sample mean is also – ya know – an unbiased estimator of the mean. The median is not (in general). What if I’m trying to estimate the population mean? What if I want to use my estimate in some bigger model? Is the distribution of the median consistent with my model’s assumptions? Or maybe I just want a robust estimate of the center of my data. Different goals will require different measures. Sure, these considerations are beyond the scope of two-weeks-into-an-intro-statistics-course, but talking with graduates of these courses never gives the impression that these complexities have been addressed at all. To students, the mean is the formula you use when your data are normalish. Otherwise, you use the other formula.\nAll courses must inevitably introduce hypothesis testing, which requires introducing the concept of a sampling distribution. This is hopeless. I am not aware of any literature actually quantifying student misunderstandings in this area, but have never in my life (and I have tried very hard) encountered a student, textbook, or instructor in the behavioral sciences with a correct, or even vaguely correct, understanding of the concept of a sampling distribution. How could it be otherwise? Consider the definition, and the depth of knowledge that it actually assumes:\nA sampling distribution is the distribution of a statistic. \nNow, approach this definition from the perspective of a student with no formal training in probability or statistics. Unpacking:\n\n\nA sampling distribution\n\n\n\nAlright, so it’s some special kind of distribution, or maybe something different from a distribution? Why does it have a special name? In reality, a sampling distribution is just a distribution. We just give it a special name when the random variable under consideration happens to have been computed from the data.\n\n\nis the distribution\n\n\n\nThe student at this point has likely never seen a density function, let alone manipulated one. Distributions are abstract concepts that basically mean that the histogram of their data will have a particular shape to it. For example, data are normally distributed if the histogram is bell shaped. The instructor says something about a mean, but I only have one mean! How can it be bell shaped? Apparently if I did a bunch of experiments it would be bell shaped, but I only did one.\n\n\nof a statistic\n\n\n\nA statistic is a function of the data, but this is simply never defined, or even alluded to. The sampling distribution is discussed only with reference to the mean. In fact, if you ask most students, they will reflexively identify the sampling distribution concept with the mean of a sample. The idea that, say, a median or any arbitrary function of the data has a sampling distribution is foreign to them. Many of them will define a sampling distribution as some variant of “the thing your mean comes from in a hypothesis test”\n\n\n\n\n\nFor obvious reasons, this definition is never provided. Instead, the instructor will conduct a thought experiment in which we hypothetically collect data many times and plot the mean of each sample. This will, the central limit theorem assures us, look bell shaped. Researchers who have been immersed in statistics for most of their working lives might not appreciate just how abstract this “drawing many hypothetical samples” business really is, but to a student with no technical background, giving them only this vague handwavy hypothetical repeated experiment business just leaves them hopelessly lost in the dark. There’s nothing for them to grapple or reason with. Certainly, they can’t use it to extrapolate beyond what they’ve been taught.\nNext, the instructor introduces a deeply important distinction. Up until now, in their ignorant savagery, the student has been dividing their sum of squared deviations from the mean by the sample size. This is called the sample variance, and it simply will not do for inference. For this, they must use the sum of squared deviations from the mean divided by the sample size minus one. This is called the population variance. This is perfectly sensible; after all, we’re interested in the population, so why not use the population variance? But what’s the difference? Well, one is the variance of the sample, and the other is the variance of the population. There will be mumbling about “degrees of freedom”, and how they lost one.\nSo, I computed the population variance of my data and it was 2.54. Does that mean it came from a population with variance 2.54? Well, no. The population variance is an estimate of the population variance. But does that mean the sample variance isn’t? Here, the student is told that the sample variance is biased. It tends to underestimate the population variance, while the population variance doesn’t. Logical, but the population variance will also be further away from the true variance, on average – it has a larger mean squared error than the sample variance. And what about standard deviation? The square root of the population variance is a biased estimate of the true standard deviation, so why do we prefer it over the square root of the sample variance? Of course, all of these properties assume that our data are normally distributed. If not, all bets are off. And all of these properties are moot when the sample size is reasonably large, since any minor difference between dividing by the sample size and dividing by the sample size minus one will be dwarfed by the measurement error and statistical uncertainty of the data. Sometimes, when I’m doing statistics in public, I like to use the sum of squared deviations from the mean divided by the sample size plus one, just to see people squirm.\nThe student is not equipped to understand any of this discussion because the student has never been introduced to the concept of an estimator. The fundamental problem of statistical inference is that our data have been generated by some process, and we want to compute something, anything, from the data that will give us knowledge of that process. If the process has a variance, we want to compute something that we hope will be reasonably close to that variance. We could conceivably compute anything – we just have to decide what properties we want our computation to have. We could want it to be right on average (unbiasedness), we could want it to be consistent across datasets (low variance), or as close as possible to the true value on average (low mean square error), or maybe robust to errors or contaminants in the data. We can choose whatever we want. The student has never been introduced to the concept of estimation – they’ve just been handed the concept of the “population variance” and told that it’s the thing they use when they’re trying to “generalize to the population”. In the future, as researchers, they don’t fully understand why they divide by the sample size minus one, they just know that it’s the proper thing to do.\nThere is a pithy way to summarize all this. Richard Feynman said the following in a 1974 address to Caltech:\n\nIn the South Seas there is a cargo cult of people. During the war they saw airplanes land with lots of good materials, and they want the same thing to happen now. So they’ve arranged to imitate things like runways, to put fires along the sides of the runways, to make a wooden hut for a man to sit in, with two wooden pieces on his head like headphones and bars of bamboo sticking out like antennas—he’s the controller—and they wait for the airplanes to land. They’re doing everything right. The form is perfect. It looks exactly the way it looked before. But it doesn’t work. No airplanes land. So I call these things cargo cult science, because they follow all the apparent precepts and forms of scientific investigation, but they’re missing something essential, because the planes don’t land.\n\nCargo cults occur when when a group imitates the aesthetics and rituals of a discipline without understanding the reasons behind them. Cargo cult statistics happens when students and researchers misinterpret the fuzzy conceptual descriptions of statistical procedures they learned as undergraduates as being literally or universally true. This usually results in a rule of thumb or layman’s description becoming morphed into a concrete statistical rule.\n“We usually divide by the sample size minus one when estimating a normal variance because it’s unbiased” becomes “You only divide by the sample size when you want to summarize your data. If you want to generalize to a population, you have to subtract one”. That many valid estimators of a normal variance exist, and that any one of them can be selected depending on the properties desired by the researcher, is lost on the student.\n“A main effect in a regression model describes the predicted change in the independent variable per unit change in the corresponding predictor when all other predictors are held constant” becomes “A main effect in a regression model is the effect of a predictor controlling for the other variables”. This leads to the dangerous idea that you can somehow “control” for a variable by including it in a regression model. In reality, the first line of control is always in the data collection process. Regression models assume very specific functional relationships between the predictors and the independent variables, and the distinction between “controlling” and “holding constant” is important.\n“A result this extreme probably wouldn’t occur under the null hypothesis” becomes “the null hypothesis is probably false”. So much has been written about this misunderstanding that it needs no further discussion.\nGeneral statistical concepts become conflated with one of their specific incarnations. Factor analysis assumes multivariate normality and is often estimated by maximum likelihood. I have been challenged several times by students who have studied factor analysis for using maximum likelihood to estimate other statistical models. You can’t use maximum likelihood to estimate a binomial probability, I have been told, because maximum likelihood assumes that the data are normal, and coin flips are not normal. Similarly, “least squares” and “linear regression” are synonymous. There is no distinction between a statistical model and the method used to estimate its parameters.\nThe word “population” is used with an almost mythical vagueness, and the ability to generalize to it possessed by only the most sophisticated of models. Consider the concepts of “fixed” and “random” effects, which are given incomprehensible explanations like “An effect is fixed if you have specified all possible levels of the effect, and random if the effects were randomly sampled from a population, or if some levels of the effects are unknown”. This inevitably followed by “fixed effect models don’t let you generalize to the population”. This is statistical gibberish, and is symptomatic of a general failure of universities to teach statistical modelling. Students do not understand that they can hypothesize any relationships they want between their variables, and formalize those relationships in the form of a statistical model. Instead, statistics is a set of procedures. If you have groups, you do an ANOVA. If your variables are continuous, you do a regression. If you want to say something about the “population”, you have to use a specific kind of regression called random effects. Fixed effects doesn’t have the “generalize to population” property. Of course, there is nothing built into either class of model that suggests such a thing. One type of model assumes that some coefficients have a particular distributional structure, whereas the other does not. Which one the researcher uses depends entirely on what they believe to be true about their data.\nThe sciences are filled with this kind of “folk wisdom” that has no real statistical or mathematical basis, but is widely believed and taught simply because it was the easiest thing for the student to grapple with when faced with the otherwise overwhelming vagary and abstraction that is statistics without any rigor or technical background. The student certainly can’t be expected to reason with or derive any concrete knowledge from “doing hypothetical experiments lots of times to get a normal distribution”, and so their understanding ends up crystallizing as a set of steps and guidelines that they vaguely remember but don’t remember why, like “divide by the sample size minus one so that I can generalize to the population”. Many university departments have not had any contact with a statistics or mathematics department in decades, and so students are taught by former students of former students who themselves have the same vague understanding.\nAs a result of all this, if you ask a student or researcher to provide a correct definition of the p-value, they cannot. The solution to this problem, says hundreds of articles in Nature or Psychological Science or the Journal of Personality and Social Psychology, is to provide them with the correct definition. But this is plainly useless – a researcher with a flawed understanding of one of the most widely used and fundamental concepts in applied statistics will not be made whole again by providing them with the proper definition of a p-value. And the solution is certainly not to provide them with even more dangerous machinery, like a Bayes factor or a posterior distribution. The whole foundation has to be removed and replaced.\n\nReferences\n\n\nHaller, H., & Krauss, S. (2002). Misinterpretations of significance: A problem students share with their teachers. Methods of Psychological Research, 7(1), 1-20.\n\nSotos, A. E. C., Vanhoof, S., Van den Noortgate, W., & Onghena, P. (2007). Students’ misconceptions of statistical inference: A review of the empirical evidence from research on statistics education. Educational Research Review, 2(2), 98-113."
  },
  {
    "objectID": "posts/2018-09-09-in-a-room-sit-three-textbooks/index.html",
    "href": "posts/2018-09-09-in-a-room-sit-three-textbooks/index.html",
    "title": "In a room sit three textbooks…",
    "section": "",
    "text": "I’ve argued before that poor statistics education in the sciences is a major contributing factor to scientists’ general lack of understanding of basic statistical concepts. I’m not so much interested in factual inaccuracies, which are inevitable, as the material has to be presented to a largely non-technical audience, but instead with how textbooks go about handling the translation of statistical concepts to a non-technical audience.\nMost results in statistics, even very basic ones, have extremely precise statements that require a rigorous understanding of concepts like a random variable or a density function, which usually have mathematical backgrounds beyond the scope of statistics courses in the sciences. This means that these concepts can’t be defined exactly correctly, but have to be given definitions which are close enough to true while still being understandable to students. This isn’t inherently negative – many practitioners of statistics need to develop a working knowledge of a concept or technique without understanding it rigorously, but simplification is a subtle art. For example, how do you simplify a definition in a way that doesn’t cause problems much later, when some subtle part of a definition which was simplified away actually becomes important? What about edge cases where the simplification doesn’t quite hold true? And how do you ensure that students understand that some simplification has occurred, and aren’t tricked into thinking that they understand a concept rigorously.\nTake the central limit theorem (CLT) as a illustration.\n\nCentral limit theorem Let \\(x_1,\\dots,x_n\\) be a random sample from a distribution with finite mean \\(\\mu\\) and variance \\(\\sigma^2\\). Then, as \\(n \\rightarrow \\infty\\), the following holds: \\[ \\sqrt{n}(\\bar{X}_n - \\mu) \\xrightarrow{d} \\mathrm{N}(0, \\sigma^2) \\] Where \\(\\xrightarrow{d}\\) denotes convergence in distribution.\n\nThere is a lot to unpack in this definition – convergence in distribution, finiteness of the mean and variance (what does it mean for a distribution to lack a mean or variance?), and the general fuzziness with which distributions and random variables are defined in many statistics courses. This means that, in the sciences, the CLT almost always has to be summarized, rather than stated (much less proved) rigorously. The goal is to give the student some kind of intuition along the lines of “if our data come from some well enough behaved distribution, then then sample mean becomes more and more normal as the sample size increases”, while introducing a minimum of false intuition or other weirdness alongside it. Sometimes this requires saying things that aren’t exactly true, but that give the right intuition, or that are true broadly enough that the student is unlikely to run into trouble. It might also involve neglecting to mention things that are true, if they add needless complexity. I thought it might be interesting to look at a few of the statistics textbooks I’ve seen used in psychology and neuroscience departments, to see how they handle the problem.\nA great example of this process done well is found in Navarro’s Learning statistics with R\n\nWhen talking about probability distributions, we’ve made use of the idea that observations X are sampled from some distribution […] This idea can be extended, to talk about the distribution of a sample statistic […] we should be able to talk about \\(\\bar{X}\\) as having a distribution. Learning statistics with R (Section 10.3, p. 300)\n\n\nThe law of large numbers is a very powerful tool, but we can actually make a stronger statement than this, using one of the most useful mathematical results in all of statistics: the central limit theorem. Let’s suppose that the population distribution of \\(X\\) has a mean \\(\\mu\\) and standard deviation \\(\\sigma\\), but it’s not a normal distribution. The central limit theorem says that, as the sample size increases, the sampling distribution of the mean \\(\\bar{X}\\) will become a normal distribution, with mean \\(\\mu\\) and standard deviation \\(\\sigma / \\sqrt{N}\\). By convention, the standard deviation of a sampling distribution is called the standard error (SE). Therefore, the standard error of the mean \\(\\bar{X}\\), can be described by the following equation \\[\n\\mathrm{se}(X) = \\frac{\\sigma}{\\sqrt{N}}\n\\] Learning statistics with R (Section 10.4, p. 301)\n\nThis is about the best overview of the CLT you’ll find in an introductory statistics textbook. Note that some details of the theorem aren’t mentioned – for example, the assumption that \\(X\\) has finite first and second moments, which is probably just a distraction at this level. It’s also a bit loose with the actual statement of the theorem, stating that \\(\\bar{X}\\) becomes normal, which a student could easily confuse to mean that normality is achieved for some finite \\(N\\). But Navarro then goes on to clarify these misunderstandings by performing simulations so that the student can actually see what is happening to the distribution of \\(\\bar{X}\\) as \\(n\\) increases.\nOne thing I really like is that the book communicates the gist of the theorem without trying to distill it down to a set of concrete rules (e.g. if the sample size is greater than 30, then we can assume that the distribution is normal), which almost always has the effect of tricking the student into thinking that their loose intuition is more rigorous than it is, simply by virtue of concrete rules seeming more rigorous, subjectively. Instead, the book does simulations from a variety of distributions and explicitly notes the differences in convergence, showing that 1) the behavior of the CLT is context dependent, and 2) the CLT gives an approximation, and we have to choose how much approximation error we are willing to tolerate in practice. Some information is lost in the simplification, but the student comes away with the understanding that there are subtleties which were not discussed, and are not tricked into believing that they understand the theorem fully.\nThe book does all of this without being condescending – just noting that the full details of the theorem are too complex to be taught in an introductory course, and then moving on with an illustration of what the theorem entails in practice. Many statistics textbooks fall into the trap of assuming that students despise the material and trying to placate them with assurances like “now don’t worry, we won’t make you do any math” or “some super smart people worked out the details, but you only have to worry about X”. I think these kinds of statements are intended to make the material more relatable, but really they just come across as insulting.\nConsider this next example, from Mayer’s Introduction to statistics and SPSS in psychology, in many ways the anti-Navarro:\n\nIn statistics, central limit theorem states that the mean of the sampling distribution equals the mean of the population and that the standard error of the mean equals the standard deviation of the population. … So long as distributions are relatively normal, we can use the principles of central limit theorem to make inferences about probability and statistical significance with relatively small samples. But how small is small? In general terms, a sample of 30 or more will probably suffice. However, we can be more precise if we make the effort to find out more about the distribution of our sample. According to central limit theorem, the sample is large enough if any of the following holds true:\n\nWhere the sample size is 15 or less: – a. The distribution must be normally distributed, – b. have no outliers and – c. must be unimodal (have one peak in the curve).\nWhere the sample size is between 16 and 40: – a. the distribution must be no more than moderately skewed, – b. have no outliers and – c. must be unimodal.\nWhere the sample size is greater than 40: – a. the distribution must have no outliers. Introduction to statistics and SPSS in psychology (2013; Ch. 4, p. 77)\n\n\nWhere to begin. First, the definition is simply wrong. That is not the statement of the central limit theorem, and it’s not even true. The standard error of the mean is most certainly not equal to the standard deviation of the population in general. What’s even more bizarre is that the book knows this – having defined the standard error only pages earlier. Neither does this discussion build any intuition for how the CLT works. Whereas Navarro carefully develops the concept of a sampling distribution, explaining that because the sample is random, anything we compute from the sample is also random, and thus has a distribution, and then shows exactly what this entails through simulation, Mayer gives a confusing attempt at a definition:\n\nTo get a better representation of that population, we could collect data from many samples. This task would be onerous, so we can use statistics to ‘model’ those theoretical samples. We call this a sampling distribution. Had we actually collected all possible samples, each sample would have a different mean and standard deviation. In a sampling distribution, we assume that the mean is the same as it is in the entire population, so long as that population is normally distributed. Introduction to statistics and SPSS in psychology (2013; Ch. 4, p. 73)\n\nwhich offers no intuition for what a sampling distribution is, and includes the bizarre statement “In a sampling distribution, we assume that the mean is the same as it is in the entire population, so long as that population is normally distributed”, which I assume is alluding to the fact that the expected value of the sample mean is equal to the mean of the population, but is phrased in such a strange way that I can’t imagine a student would be able to parse it, given that “sampling distribution” hasn’t even been defined beyond the claim that we somehow use it to model “theoretical samples”. Without making clear that the mean actually has a distribution, the book can’t hope to develop any intuitive understanding of the CLT, so (after giving an incorrect statement of the theorem) the book goes on to list several rules. “According to the central limit theorem, the sample is large enough”, if it has at least 40 observations, but we can get away with 15 if the sample is normal. But then why do we need the CLT? If the sample is normally distributed, the mean is already normal, so we don’t need the CLT at all. Of course, the CLT doesn’t actually provide any of these “rules”. Whereas Navarro is honest that its definition is not rigorous, and instead tries to develop intuition by showing how the CLT works in practice, Mayer gives no intuition at all, and lies to the student by giving a definition and a set of rules that appear to be rigorous, but are actually false.\nAnother common textbook, Gravetter’s Statistics for the Behavioral Sciences does a little bit better\n\n[A] mathematical proposition known as the central limit theorem provides a precise description of the distribution that would be obtained if you selected every possible sample, calculated every sample mean, and constructed the distribution of the sample mean. This important and useful theorem serves as a cornerstone for much of inferential statistics. Following is the essence of the theorem\nCentral limit theorem: For any population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the distribution of sample means for sample size \\(n\\) will have a mean of \\(\\mu\\) and a standard deviation of \\(\\sigma/\\sqrt{n}\\) and will approach a normal distribution as \\(n\\) approaches infinity.\nThe value of this theorem comes from two simple facts. First, it describes the distribution of sample means for any population, no matter what shape, mean, or standard deviation. Second, the distribution of sample means “approaches” a normal distribution very rapidly. By the time the sample size reaches \\(n = 30\\), the distribution is almost perfectly normal. Statistics for the Behavioral Sciences (9th ed. Ch. 7, p. 205)\n\nThis is…eh. It at least attempts to describe the concept of a sampling distribution, and the statement of the CLT is true-ish, through not nearly as “usefully wrong” as Navarro, which made it clear when it was deviating from the rigorous statement of the theorem, and generally did so only when it was necessary to avoid confusion. Instead, Gravetter confidently states things like “By the time the sample size reaches \\(n = 30\\), the distribution is almost perfectly normal”, which is both not true and not helpful.\nI’ve deliberately avoided mentioning the “significance issue”, since I’m more interested in how textbooks approach the presentation of material to a non-technical audience than I am with misinterpretations of significance testing specifically, but it’s worthwhile to see how each book covers the material. I find that most textbooks present significance testing in a way which is technically correct, but encourages the false intuition that the p-value is somehow a measure of the probability that the observed results are “true”. Gravetter is good about this. It develops the logic of significance testing properly:\n\nA significant result permits the following conclusion: “This specific sample mean is very unlikely (\\(p &lt; .05\\)) if the null hypothesis is true.” Statistics for the Behavioral Sciences (9th ed. Ch. 8, p. 260)\n\nand then devotes an entire section to dispelling the misunderstanding that the p-value quantifies the probability of the null hypothesis. I’d have to say that, despite devoting far too much time to significance testing, which I don’t think is very useful in practice, Gravetter actually covers the logic of significance testing very well. Unfortunately, it falls back into common habits when discussing confidence intervals:\n\n…we can confidently estimate that the value of the parameter should be located in the interval. Statistics for the Behavioral Sciences (9th ed. Ch. 9, p. 300)\n\nOn the other hand, Mayer is just a disaster:\n\nBy stating that there is less than 5% probability that an outcome occurred by chance, we are actually saying that there is a less than 5% probability that the null hypothesis is “true”. Introduction to statistics and SPSS in psychology (2013; Ch. 4, p. 68)\n\nAbsolutely not.\n\nA key factor to remember with hypothesis testing is that we are dealing with probability, not certainty. Statistics will only tell us the likelihood that the outcome occurred by chance. Introduction to statistics and SPSS in psychology (2013; Ch. 4, p. 70)\n\nNo no no.\n\nIn this first example we will demonstrate how we can calculate the achieved statistical power, based on outcomes from a completed study. Introduction to statistics and SPSS in psychology (2013; Ch. 4, p. 84)\n\nGod no. Honestly, I’m a little disturbed to see Mayer released by such a major publisher (Pearson). I find it very difficult to follow, even having a strong background in the material. The book uses terminology in extremely confusing and non-standard ways. For example, it seems to conflate the standard error of the sample mean with the standard deviation of the population, or maybe it uses “population” to mean the distribution of sample means under repeated experiments? It’s hard to tell:\n\n…the standard error of the mean equals the standard deviation of the population. Introduction to statistics and SPSS in psychology (2013; Ch. 4, p. 77)\n\n\nAs we saw earlier, the population is potentially infinite, so we need to estimate the standard deviation of the population from the standard error. Introduction to statistics and SPSS in psychology (2013; Ch. 4, p. 78)\n\n\nStandard Error: the average variation of scores in a sampling distribution, or the estimated variation in the population. Introduction to statistics and SPSS in psychology (2013; Ch. 4, p. 80)\n\nThis isn’t a trivial mistake. The standard deviation and standard error are not only not equal, they don’t even quantify the variability of the same distributions. By confusing the concepts of sample, sampling distribution, and population, the book makes it impossible to understand any one of those concepts individually, which makes it impossible to understand any technique that references those concepts.\nFinally, Navarro takes the approach of working out a binomial example in painstaking detail before introducing the general concept of significance testing. I like this, since it makes it absolutely clear what is being calculated, and leaves no room for misinterpretation (though later it does very explicitly address the common misinterpretations of significance testing). The book then goes on to describe in detail the conflicting Neyman and Fisher approaches to statistical testing, though I’m not sure if this would help the student or just confuse them.\nThough only three, I’d argue that these books give a pretty good idea of the general classes of books that are commonly aimed towards students in the sciences: books which are broadly or technically correct, but don’t really give good intuition, and don’t accurately portray how statistics is done in practice; books which are just plain wrong; and books or lecture notes, almost always independent or open source efforts, which portray the material in a way which is technically correct, and try desperately to convey the right intuition, but still contend with having to inoculate students against bad practices in other books and in the literature. This means that even good books like Nevarro have to waste entire chapters on significance testing just because students have to understand it thoroughly in order to read the literature. To its credit, it relegates the material to a single chapter (Gravetter devotes three chapters to the t-test! Why?!), and devotes an entire chapter early on to estimation (the only introductory textbook in the sciences to do so, as far as I know). The remainder of the book is dedicated to regression and ANOVA as models, and not simply as elaborate versions of the t-test, which is great. It’s a very minor point, but Nevarro is also the only book to make a distinction between regression as a model and least-squares estimation. I think this distinction between a statistical model and the technique used to estimate it is actually very important when building intuition for modeling more generally.\nI wrote this post out of curiosity, as I wanted to see how statistics was being taught here at Queens, and I ended up picking the first three books I found in stacks of old course outlines. I guess you could interpret this as an endorsement of Nevarro’s Learning statistics with R, but really, I think Nevarro is just the closest you can get to good given the required constraints for statistics courses in fields where statistics just isn’t used very well. I once took a course in Topology taught by the Moore method, and the more I think about it, the more I think it’s the only right way to approach data analysis more generally. Give students datasets and have them devise methods to answer questions about them; have other students try to break those methods, or identify boundary cases or limitations; then have them devise ways to test or verify their models. No mass-univariate significance testing silliness would ever survive this kind of Darwinism."
  },
  {
    "objectID": "posts/2019-05-26-spurious-dynamics/index.html",
    "href": "posts/2019-05-26-spurious-dynamics/index.html",
    "title": "Spurious dynamics in functional brain networks",
    "section": "",
    "text": "Network neuroscience (NN) as a field lacks statistical and theoretical justification for almost any of it’s techniques. The discipline consists largely of ad hoc measures whose statistical properties are very poorly understood and whose relationships to brain function (if they exist at all) have not been clearly established. Much of this stems from a lack of domain knowledge – network neuroscience is a very young field – but a part of it comes from a lack of methodological rigour in general. Most techniques in NN have been imported from other fields (esp. social network analysis), and the most influential papers read like the appendix to an introductory graph theory textbook, with the authors computing every graph property they can think of with no attempt to establish the meaning of the property with respect to the brain, or even whether the property can be reliably estimated from a sample at all (i.e. the field lacks a statistical framework in general.)\nDynamic networks are particularly tricky, as much of the field has been developed in the context of resting-state brain activity, where there are no exogenous variables that can be used to model the network dynamics. This has led – very broadly – to two principal approaches to the study of dynamic connectivity: 1) What I’ll call the “modeling” approach, exemplified by something like the markov switching factor models of e.g. Taghia et al. (2017), and 2) the “descriptive” approach, exemplified by authors like Bassett et al. (2011, 2013) and Betzel et al. (2013), which analyzes descriptive statistics computed from static or dynamic graphs.\nThe construction of these graphs generally requires a large number of arbitrary preprocessing choices, and there is very little understanding of the ways in which these choices impact the resulting statistics. Much of the existing research is satisfied to compute a network measure and establish it’s significance relative to a random graph, which is usually accomplished by some form of permutation testing, since we lack: 1) a sensible null model for the absence of dynamic functional connectivity; 2) an understanding of how this null model changes under various preprocessing decisions; and 3) a null distribution for most of our network measures under (1) and (2). This is far too broad of a subject to unpack in a single post, so I’ll tackle a single, specific example.\nWe often construct temporal networks from resting scans (or any other scan) using temporal sliding windows. Naturally, we observe changes from window to window – if, for no other reason, than because the data are noisy. We of course want to know whether this temporal structure reflects genuine changes in functional connectivity over time, or if it is explainable by noise alone. Lacking a theoretical null model, one possible approach to conducting this test is through the temporal null model of Bassett, et al. 2013 – a permutation test in which the modularity (or some other statistic) of the observed network is compared to a null distribution generated by randomly permuting the temporal order of the network slices, in order to “…probe the existence of significant temporal evolution of community structure” (p. 9)\nThe conceptual null model here is that true dynamic functional connectivity varies smoothly over time. Shuffling the window order destroys this temporal smoothness while preserving other quantities, such as the variance in functional connectivity over the course of the scan. This test is thus insensitive to the overall range of functional connectivity, as this is preserved by temporal shuffling. This is not a failure of the test – it’s just always good to keep in mind exactly what is being tested.\nThe problem is that several preprocessing decisions can introduce temporal smoothness into the observed network, and this test turns out to be wildly miscalibrated under these settings. I’ll focus here on two: the use of overlapping time windows, and the use of low-pass filtering on the BOLD signal (more generally, the presence of autocorrelation). The reason is fairly intuitive – both of these actions introduce sequential dependence in adjacent networks that is destroyed by time randomization. This sequential dependence creates the appearance of temporal smoothness beyond what would be expected from windows containing independent observations. This is easy to see by simulation.\nWe construct temporal networks from uncorrelated white noise signals using a sliding window approach. Within each window, we create a weighted graph by thresholding the sample correlation matrix at zero. The community structure of the resulting temporal network is then estimated using the generalized Louvain algorithm implemented in the MATLAB toolbox provided by Lucas et al. (2011-2017). Each iteration is as follows:\ninput: num_sim = 100\n       num_roi = 100\n       num_timepoint = 300\n       window_width = 50\n       stepsize = [50 40 30 20 10]\n       smooth = [2 5 10 15]\n       n_iter = 50 \n       n_shuffle = 50\n       gamma = 1\n       omega = 1\n\nFor 1:num_sim\n\n1) Generate a \"num_roi\" x \"num_timepoint\" matrix \n   of independent, standard normal random numbers.\n\n2) Low pass filter each time series, with filter\n   parameter \"smooth\"\n\n3) Using a sliding window of length \"window_width\", \n   moving in steps of \"stepsize\", calculate a \n   sequence of sample correlation matrices.\n\n4) Set negative correlations to zero.\n\n5) For i = 1:n_iter\n   \n   5a) Construct a multilayer modularity matrix\n       with multiord(..., gamma, omega)\n\n   5b) Estimate community structure with\n       iterated_genlouvain(..., moverandw, \n           postprocess_ordinal_multilayer)\n\n   5c) Compute network statistics\n\n6) Compute average statistic over n_iter iterations\n\n7) Repeat (5-6) for \"n_shuffle\" randomly shuffled\n   sequences. \n\n8) Compute z-statistic for (6)\n\nIn this case, low-pass filtering is crudely done with a moving average filter of radius smooth. The output of (7) is used as a temporal null distribution to derive a z-statistic for the network. I’ll focus here on the modularity (Q), as it is one of the most commonly reported statistics in the literature. The resulting z-scores and type-1 error rates are shown below, plotted as a function of smooth and the proportion of window overlap.\n\n\n\n\n\n\n\n\n\n\nClearly, overlapping windows result in a grossly inflated type-1 error rate, and inflated modularity in general. The effect of smoothing is less clear, and may just need a larger number of iterations for more precision, though a simple ANOVA gives evidence for some effect\n&gt; fit &lt;- aov(z ~ Smooth * Overlap, data = af)\n&gt; summary(fit)\n                Df Sum Sq Mean Sq  F value  Pr(&gt;F)    \nSmooth           1     11      11    7.620 0.00588 ** \nOverlap          1   4098    4098 2970.096 &lt; 2e-16 ***\nSmooth:Overlap   1      0       0    0.295 0.58743    \nResiduals      996   1374       1                     \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nThe lesson here is that one should be very precise about the quality that is being tested (in this case, temporal smoothness), and either avoid preprocessing which introduces this quality into the data, or construct a test which is insensitive to these preprocessing decisions.\n\nReferences\n\n\nBassett, D. S., Porter, M. A., Wymbs, N. F., Grafton, S. T., Carlson, J. M., & Mucha, P. J. (2013). Robust detection of dynamic community structure in networks. Chaos: An Interdisciplinary Journal of Nonlinear Science, 23(1), 013142.\n\n\nBassett, D. S., Wymbs, N. F., Porter, M. A., Mucha, P. J., Carlson, J. M., & Grafton, S. T. (2011). Dynamic reconfiguration of human brain networks during learning. Proceedings of the National Academy of Sciences, 108(18), 7641-7646.\n\n\nBetzel, R. F., Griffa, A., Avena-Koenigsberger, A., Goñi, J., Thiran, J. P., Hagmann, P., & Sporns, O. (2013). Multi-scale community organization of the human structural connectome and its relationship with resting-state functional connectivity. Network Science, 1(3), 353-373.\n\n\nLucas G. S. Jeub, Marya Bazzi, Inderjit S. Jutla, and Peter J. Mucha, “A generalized Louvain method for community detection implemented in MATLAB,” http://netwiki.amath.unc.edu/GenLouvain (2011-2017).\n\n\nTaghia, J., Ryali, S., Chen, T., Supekar, K., Cai, W., & Menon, V. (2017). Bayesian switching factor analysis for estimating time-varying functional connectivity in fMRI. NeuroImage, 155, 271-290."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Corson N. Areshenkoff",
    "section": "",
    "text": "I am a researcher at the Centre for Neuroscience Studies at Queens University, working in the Memory, Action, and Perception Lab under Dr. Jason Gallivan. My background is in statistics and mathematics, and I typically enjoy projects that lend themselves to interesting or unusual analyses. Although I’ve dabbled in many areas, my general focus is on the use of functional neuroimaging methods to study brain networks supporting motor learning processes.\n\nEducation\n\nPhD Psychology, Queens University, 2023\n\nThesis: Taking the ‘motor’ out of motor learning: The role of cognitive brain networks and explicit processes in sensorimotor adaptation.\n\nMSc Psychology, University of Victoria, 2016\n\nThesis: Task-dependent motor representations evoked by spatial words\n\nBSc Mathematics, University of Victoria, 2015"
  },
  {
    "objectID": "packages/iowa.html",
    "href": "packages/iowa.html",
    "title": "iowa",
    "section": "",
    "text": "The iowa package implements modular reinforcement learning models of the Iowa gambling task, using RStan as a backend.\nThe package allows both the simulation and fitting of models constructed by mixing and matching various utility, updating, and temperature functions; and is designed to be relatively extensible by allowing users to implement custom model components and deck structures, which may then be used relatively seamlessly with the functions already implemented in the package.\nIn addition to simulating the performance of custom models, iowa also allows model fitting either by maximum likelihood / maximum a posteriori estimation, or by full posterior sampling. Currently, only single subject fitting is supported, but implementing support for full hierarchical Bayesian fitting is a strong priority.\nFor more information, see the documentation.\n\nInstallation\niowa can be installed from GitHub with using install_git() from the devtools package using the command below:\ndevtools::install_git('areshenk-rpackages/iowa')"
  }
]